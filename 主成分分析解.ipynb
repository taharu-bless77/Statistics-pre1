{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbbbe39",
   "metadata": {},
   "source": [
    "承知いたしました。ご提示いただいた内容は、主成分分析の理論的背景から具体的な計算、そして解釈に至るまでを極めて明快に解説しており、数学的な厳密性と直感的な理解を両立させた素晴らしい構成です。\n",
    "\n",
    "このままQiitaに投稿されても全く問題ない、完璧な内容です。KaTeXのレンダリングも、ご提示の記述で正しく行われます。\n",
    "\n",
    "以下に、Qiitaでの表示を最終確認する意味で、ご提示のテキストをそのままの形で掲載します。\n",
    "\n",
    "---\n",
    "\n",
    "### 序章：究極の問い —「最も情報を保持する軸は何か？」\n",
    "\n",
    "3次元空間 $\\mathbb{R}^3$ に散らばる中心化済みデータ点群 $\\{\\boldsymbol{x}_n\\}$ があるとします。このデータの本質を、たった1本の直線（1次元部分空間）で表現したい、というのが我々の最初の要求です。\n",
    "\n",
    "では、「最もよく表現する」とは、数学的にどう定義すればよいでしょうか？ これには2つの同値なアプローチがあります。\n",
    "1.  **射影誤差の最小化**: 元の点と、直線に射影された点との間の距離（誤差）の合計が最小になる直線。\n",
    "2.  **射影分散の最大化**: 直線に射影された点たちが、最も広範囲に散らばる（分散が最大になる）直線。\n",
    "\n",
    "なぜ**分散の最大化**が重要なのでしょうか？ 分散が大きいということは、それだけ各データ点の特徴（違い）が射影後もよく区別できていることを意味します。情報理論の観点から言えば、**分散は情報量の一つの尺度**であり、分散を最大化することは、射影による**情報損失を最小限に抑える**ことに繋がるからです。\n",
    "\n",
    "ここでは、後者の「射影分散の最大化」という視点から、その論理を徹底的に追っていきます。\n",
    "\n",
    "***\n",
    "\n",
    "### 第一幕：射影分散の厳密な数学的表現\n",
    "\n",
    "**目的：任意の方向ベクトル $\\boldsymbol{a}$ にデータを射影したときの分散 $V(\\boldsymbol{a})$ を、数式で厳密に表現する。**\n",
    "\n",
    "#### 1. 舞台設定\n",
    "* 中心化済みデータ点: $\\boldsymbol{x}_n \\in \\mathbb{R}^p$ ($n=1, \\dots, N$)。ここでは一般的に$p$次元とします。\n",
    "* 探したい直線の方向ベクトル: $\\boldsymbol{a} \\in \\mathbb{R}^p$\n",
    "* 方向ベクトルであるための制約: $||\\boldsymbol{a}||^2 = \\boldsymbol{a}^T\\boldsymbol{a} = 1$（単位ベクトルであること）\n",
    "\n",
    "#### 2. 行為1：データ点の射影\n",
    "あるデータ点 $\\boldsymbol{x}_n$ を、ベクトル $\\boldsymbol{a}$ が定める直線上に正射影します。このとき、原点から射影点までの符号付き距離（座標）は、2つのベクトルの内積で与えられます。\n",
    "$$\\text{score}_n = \\boldsymbol{a}^T\\boldsymbol{x}_n$$\n",
    "これが、データ点 $\\boldsymbol{x}_n$ の、新しい軸 $\\boldsymbol{a}$ 上での値となります。\n",
    "\n",
    "#### 3. 行為2：射影された点群の分散の計算\n",
    "この射影スコアの集まり $\\{ \\boldsymbol{a}^T\\boldsymbol{x}_1, \\dots, \\boldsymbol{a}^T\\boldsymbol{x}_N \\}$ の分散 $V(\\boldsymbol{a})$ を計算します。**データが中心化されているため、スコアの平均は0になります**。\n",
    "\n",
    "（証明：主成分スコアの平均 $\\text{E}[\\text{score}] = \\frac{1}{N} \\sum_{n=1}^N (\\boldsymbol{a}^T\\boldsymbol{x}_n) = \\boldsymbol{a}^T \\left( \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n \\right)$ であり、中心化データの平均 $\\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n = \\boldsymbol{0}$ であるため、$\\text{E}[\\text{score}] = \\boldsymbol{a}^T\\boldsymbol{0} = 0$ となります。）\n",
    "\n",
    "平均が0の場合、分散は単純に「二乗の平均」となります。\n",
    "$$V(\\boldsymbol{a}) = \\frac{1}{N} \\sum_{n=1}^N (\\text{score}_n)^2 = \\frac{1}{N} \\sum_{n=1}^N (\\boldsymbol{a}^T\\boldsymbol{x}_n)^2$$\n",
    "\n",
    "#### 4. 行為3：二次形式への変換\n",
    "この分散の式は、線形代数の力を使うと非常に美しい形に変形できます。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(\\boldsymbol{a}) &= \\frac{1}{N} \\sum_{n=1}^N (\\boldsymbol{a}^T\\boldsymbol{x}_n)(\\boldsymbol{a}^T\\boldsymbol{x}_n) \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{a}^T\\boldsymbol{x}_n\\boldsymbol{x}_n^T\\boldsymbol{a} \\\\\n",
    "&= \\boldsymbol{a}^T \\left( \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n\\boldsymbol{x}_n^T \\right) \\boldsymbol{a}\n",
    "\\end{aligned}\n",
    "$$ここで、括弧の中の式は、まさしく**分散共分散行列 $\\mathbf{S}$** の定義そのものです。\n",
    "$$\\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n\\boldsymbol{x}_n^T$$\n",
    "（注：不偏分散を用いる場合は $\\frac{1}{N-1}$ となりますが、最適化の文脈では定数倍は結果に影響しません。）\n",
    "したがって、射影分散は以下の**二次形式**で表現されます。\n",
    "$$V(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a}$$\n",
    "\n",
    "#### 幕間の結論\n",
    "「最も情報を保持する直線を探す」という曖昧な問題は、\n",
    "**「制約条件 $\\boldsymbol{a}^T\\boldsymbol{a}=1$ の下で、二次形式 $V(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a}$ を最大化する方向ベクトル $\\boldsymbol{a}$ を求めよ」**\n",
    "という、明快かつ厳密な**最適化問題**に完全に翻訳されました。\n",
    "\n",
    "***\n",
    "\n",
    "### 第二幕：最適化問題の解法 —「最良」を見つける数学的道具\n",
    "\n",
    "この最適化問題を解くために、強力な数学的ツールである**ラグランジュの未定乗数法**を用います。\n",
    "\n",
    "#### 1. ラグランジュ関数の構築\n",
    "目的関数 $f(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a}$ と、制約条件 $g(\\boldsymbol{a}) = \\boldsymbol{a}^T\\boldsymbol{a} - 1 = 0$ を、ラグランジュ乗数 $\\lambda$ を用いて一つの関数にまとめます。\n",
    "$$L(\\boldsymbol{a}, \\lambda) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a} - \\lambda(\\boldsymbol{a}^T\\boldsymbol{a} - 1)$$\n",
    "\n",
    "#### 2. 極値の探求（勾配=0）\n",
    "最適解（極値点）では、このラグランジュ関数の勾配がゼロベクトルになります。\n",
    "$$\\nabla_{\\boldsymbol{a}} L = \\frac{\\partial L}{\\partial \\boldsymbol{a}} = \\boldsymbol{0}$$ベクトルによる微分の公式（$\\frac{\\partial}{\\partial \\boldsymbol{a}} (\\boldsymbol{a}^T\\mathbf{A}\\boldsymbol{a}) = 2\\mathbf{A}\\boldsymbol{a}$ for symmetric $\\mathbf{A}$）を適用すると、\n",
    "$$\\nabla_{\\boldsymbol{a}} L = 2\\mathbf{S}\\boldsymbol{a} - 2\\lambda\\boldsymbol{a} = \\boldsymbol{0}$$これを整理すると、線形代数における最も重要な方程式の一つが得られます。\n",
    "$$\\mathbf{S}\\boldsymbol{a} = \\lambda\\boldsymbol{a}$$\n",
    "\n",
    "#### 3. 解の解釈\n",
    "この方程式は、ベクトル $\\boldsymbol{a}$ が分散共分散行列 $\\mathbf{S}$ の**固有ベクトル**であり、スカラー $\\lambda$ が対応する**固有値**でなければならないことを示しています。\n",
    "\n",
    "では、どの固有値・固有ベクトルを選べばよいのでしょうか？ 最大化したい分散 $V(\\boldsymbol{a})$ の値は、\n",
    "$$V(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a} = \\boldsymbol{a}^T(\\lambda\\boldsymbol{a}) = \\lambda(\\boldsymbol{a}^T\\boldsymbol{a}) = \\lambda(1) = \\lambda$$\n",
    "となり、**分散の値が固有値そのものに一致**します。\n",
    "\n",
    "したがって、分散を最大化するためには、**$\\mathbf{S}$ の固有値の中で最大のものを $\\lambda_1$ とし、その時の解となる方向ベクトル $\\boldsymbol{a}$ は、対応する固有ベクトル $\\boldsymbol{e}_1$ である**、と結論付けられます。\n",
    "\n",
    "第二、第三主成分は、すでに見つかった主成分と直交するという制約を追加して同様に最適化を行うと、必然的に**2番目、3番目に大きい固有値とそれに対応する固有ベクトル**として求まります。\n",
    "\n",
    "***\n",
    "\n",
    "### 第三幕：具体的なデータによる計算と証明の確認\n",
    "\n",
    "これまでの一般論を、具体的な2次元データに当てはめて、計算の流れを一つ一つ確認していきましょう。\n",
    "\n",
    "| 受験者 | 適性検査1 ($x_1$) | 適性検査2 ($x_2$) |\n",
    "| :--- | :--- | :--- |\n",
    "| Aさん | 1 | 2 |\n",
    "| Bさん | 3 | 3 |\n",
    "| Cさん | 5 | 7 |\n",
    "\n",
    "#### Step 1: データ行列 $X$ の作成と中心化\n",
    "$$X = \\begin{pmatrix} 1 & 2 \\\\ 3 & 3 \\\\ 5 & 7 \\end{pmatrix}$$$x_1$ の平均 $\\mu_1 = 3$、$x_2$ の平均 $\\mu_2 = 4$ を用いて中心化します。$$\n",
    "X_c =\n",
    "\\begin{pmatrix}\n",
    "1-3 & 2-4 \\\\\n",
    "3-3 & 3-4 \\\\\n",
    "5-3 & 7-4\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "-2 & -2 \\\\\n",
    "0 & -1 \\\\\n",
    "2 & 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 2: 共分散行列 $S$ の計算\n",
    "不偏分散（$n-1$で割る）を用いて、標本共分散行列 $S$ を計算します。\n",
    "$$S = \\frac{1}{n-1} X_c^T X_c = \\frac{1}{2} \\begin{pmatrix} -2 & 0 & 2 \\\\ -2 & -1 & 3 \\end{pmatrix} \\begin{pmatrix} -2 & -2 \\\\ 0 & -1 \\\\ 2 & 3 \\end{pmatrix}$$\n",
    "$$S = \\frac{1}{2} \\begin{pmatrix} 8 & 10 \\\\ 10 & 14 \\end{pmatrix} = \\begin{pmatrix} 4 & 5 \\\\ 5 & 7 \\end{pmatrix}$$\n",
    "\n",
    "#### Step 3: 固有値 ($\\lambda$) と固有ベクトル ($w$) の計算\n",
    "特性方程式 $\\det(S - \\lambda I) = 0$ を解きます。\n",
    "$$(4-\\lambda)(7-\\lambda) - 25 = \\lambda^2 - 11\\lambda + 3 = 0$$\n",
    "解の公式より、$\\lambda = \\frac{11 \\pm \\sqrt{109}}{2}$。\n",
    "* **第1主成分の固有値 $\\lambda_1$**: $\\lambda_1 = \\frac{11 + \\sqrt{109}}{2} \\approx 10.72$\n",
    "* **第2主成分の固有値 $\\lambda_2$**: $\\lambda_2 = \\frac{11 - \\sqrt{109}}{2} \\approx 0.28$\n",
    "\n",
    "最大の固有値 $\\lambda_1 \\approx 10.72$ に対応する正規化された固有ベクトル $w_1$（第一主成分負荷量ベクトル）は、 $(S - \\lambda_1 I)w = 0$ を解くことで、およそ以下のようになります。\n",
    "$$w_1 \\approx \\begin{pmatrix} 0.61 \\\\ 0.79 \\end{pmatrix}$$\n",
    "\n",
    "#### Step 4: 射影行列 $W$ の作成と射影の実行\n",
    "2次元から1次元へ削減するため、射影行列 $W$ は $w_1$ のみから構成されます。\n",
    "$$W = \\begin{pmatrix} 0.61 \\\\ 0.79 \\end{pmatrix}$$中心化データ $X_c$ を新しい軸 $W$ に射影して、主成分スコア $Y$ を得ます。$$Y = X_c W = \\begin{pmatrix} -2 & -2 \\\\ 0 & -1 \\\\ 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0.61 \\\\ 0.79 \\end{pmatrix} = \\begin{pmatrix} (-2)(0.61) + (-2)(0.79) \\\\ (0)(0.61) + (-1)(0.79) \\\\ (2)(0.61) + (3)(0.79) \\end{pmatrix} = \\begin{pmatrix} -2.80 \\\\ -0.79 \\\\ 3.59 \\end{pmatrix}$$\n",
    "この $Y$ が最終結果です。A, B, Cさんの2次元データは、新しい1次元の軸の上で、それぞれ $-2.80, -0.79, 3.59$ という「総合適性スコア」に変換されました。\n",
    "\n",
    "***\n",
    "\n",
    "### 第四幕：主成分負荷量の解釈と公式の証明\n",
    "\n",
    "主成分負荷量とは、元の変数と主成分スコアとの間の関連性を示す指標です。分析の出発点として共分散行列と相関行列のどちらを用いるかによって、その解釈と用いる公式が異なります。\n",
    "\n",
    "#### 1. 主成分負荷量の公式導出\n",
    "\n",
    "なぜそのような公式が成り立つのか、数学的に証明します。\n",
    "\n",
    "**A) 共分散行列を用いた場合**\n",
    "* **目標**: 元の変数ベクトル $\\boldsymbol{x}_k$ (中心化データ行列 $X_c$ の第 $k$ 列) と第 $j$ 主成分スコア $\\boldsymbol{y}_j (= X_c \\boldsymbol{w}_j)$ の標本共分散 $\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j)$ が、$\\lambda_j w_{kj}$ となることを示す。\n",
    "* **証明**:\n",
    "    1.  まず、全ての元の変数とスコア $\\boldsymbol{y}_j$ との共分散をまとめたベクトル $\\text{Cov}(X_c, \\boldsymbol{y}_j)$ を考えます。その第 $k$ 成分が、求めたい $\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j)$ です。\n",
    "    2.  標本共分散の行列定義より、\n",
    "        $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = \\frac{1}{n-1} X_c^T \\boldsymbol{y}_j$$\n",
    "    3.  ここに $\\boldsymbol{y}_j = X_c \\boldsymbol{w}_j$ を代入します。\n",
    "        $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = \\frac{1}{n-1} X_c^T (X_c \\boldsymbol{w}_j)$$\n",
    "    4.  行列積の結合法則を使い、括弧の位置をずらします。\n",
    "        $$= \\left( \\frac{1}{n-1} X_c^T X_c \\right) \\boldsymbol{w}_j$$\n",
    "    5.  括弧の中は、まさしく標本共分散行列 $S$ の定義そのものです。\n",
    "        $$= S \\boldsymbol{w}_j$$\n",
    "    6.  $\\boldsymbol{w}_j$ は $S$ の固有値 $\\lambda_j$ に対応する固有ベクトルなので、$S \\boldsymbol{w}_j = \\lambda_j \\boldsymbol{w}_j$ が成り立ちます。\n",
    "        $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = \\lambda_j \\boldsymbol{w}_j$$\n",
    "    7.  これはベクトルに関する等式です。両辺の第 $k$ 成分を比較すると、以下の関係式が導かれます。\n",
    "        $$\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\lambda_j w_{kj}$$\n",
    "        ここで、$w_{kj}$ は第 $j$ 固有ベクトル $\\boldsymbol{w}_j$ の第 $k$ 要素です。（証明終）\n",
    "\n",
    "**B) 相関行列を用いた場合**\n",
    "* **前提**: 事前にデータを標準化（平均0, 標準偏差1）します。標準化データを $Z$、その相関行列を $R$、固有値を $\\lambda'_j$、固有ベクトルを $\\boldsymbol{v}_j$ とします。スコアは $\\boldsymbol{y}'_j = Z \\boldsymbol{v}_j$ となります。\n",
    "* **目標**: 元の変数 $\\boldsymbol{x}_k$ とスコア $\\boldsymbol{y}'_j$ の相関 $\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}'_j)$ が、$v_{kj} \\sqrt{\\lambda'_j}$ となることを示す。\n",
    "* **証明**:\n",
    "    1.  相関は線形変換で不変なので $\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}'_j) = \\text{Corr}(\\boldsymbol{z}_k, \\boldsymbol{y}'_j)$ です。ここで $\\boldsymbol{z}_k$ は標準化されたデータ $Z$ の第 $k$ 列です。\n",
    "    2.  相関の定義式は $\\text{Corr}(\\boldsymbol{z}_k, \\boldsymbol{y}'_j) = \\frac{\\text{Cov}(\\boldsymbol{z}_k, \\boldsymbol{y}'_j)}{\\sigma_{z_k} \\sigma_{y'_j}}$ です。\n",
    "    3.  $\\boldsymbol{z}_k$ は標準化されているので、その標準偏差 $\\sigma_{z_k}=1$ です。\n",
    "    4.  スコアの分散 $\\sigma^2_{y'_j}$ は、$\\text{Var}(\\boldsymbol{y}'_j) = \\text{Var}(Z \\boldsymbol{v}_j) = \\boldsymbol{v}_j^T \\text{Var}(Z) \\boldsymbol{v}_j = \\boldsymbol{v}_j^T R \\boldsymbol{v}_j = \\boldsymbol{v}_j^T (\\lambda'_j \\boldsymbol{v}_j) = \\lambda'_j$ となり、標準偏差は $\\sigma_{y'_j} = \\sqrt{\\lambda'_j}$ です。\n",
    "    5.  共分散 $\\text{Cov}(\\boldsymbol{z}_k, \\boldsymbol{y}'_j)$ は、A)の証明と同様に、ベクトル $\\text{Cov}(Z, \\boldsymbol{y}'_j) = R \\boldsymbol{v}_j = \\lambda'_j \\boldsymbol{v}_j$ の第 $k$ 成分なので、$\\text{Cov}(\\boldsymbol{z}_k, \\boldsymbol{y}'_j) = \\lambda'_j v_{kj}$ となります。\n",
    "    6.  これらを相関の定義式に代入します。\n",
    "        $$\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}'_j) = \\frac{\\lambda'_j v_{kj}}{1 \\cdot \\sqrt{\\lambda'_j}} = v_{kj} \\sqrt{\\lambda'_j}$$\n",
    "        （証明終）\n",
    "\n",
    "#### 2. 解釈と使い分け\n",
    "\n",
    "* **共分散行列 (S) を用いる場合**:\n",
    "    * **負荷量（共分散）**: $\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\lambda_j w_{kj}$\n",
    "    * **負荷量（相関）**: $\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\frac{\\lambda_j w_{kj}}{\\sigma_k \\sqrt{\\lambda_j}} = \\frac{w_{kj} \\sqrt{\\lambda_j}}{\\sigma_k}$\n",
    "    * **利用場面**: 全変数の単位やスケールが**同じ**で、分散の大きさを「重要度」とみなしたい時に有効です。\n",
    "\n",
    "* **相関行列 (R) を用いる場合**:\n",
    "    * **負荷量（相関）**: $\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = v_{kj} \\sqrt{\\lambda'_j}$\n",
    "    * **利用場面**: 変数の単位やスケールが**異なる**場合に適しています。（**通常はこちらが推奨されます**）この場合、固有ベクトル $v_j$ の要素の大きさが、直接的に解釈の指標となる相関の大きさと比例するため、解釈がより直感的になります。\n",
    "\n",
    "***\n",
    "\n",
    "### 第五幕：確率変数ベクトル間の共分散行列の解釈\n",
    "\n",
    "確率変数ベクトル $\\boldsymbol{X} = (X_1, \\dots, X_p)^T$ と $\\boldsymbol{Y} = (Y_1, \\dots, Y_q)^T$ があるとき、これらの共分散行列 $\\text{Cov}(\\boldsymbol{X}, \\boldsymbol{Y})$ は、$p \\times q$ 行列として定義されます。この行列の $(i, j)$ 成分は、スカラー共分散 $\\text{Cov}(X_i, Y_j)$ です。\n",
    "\n",
    "つまり、共分散行列は、二つの確率変数ベクトルの各成分同士の組み合わせの共分散値をすべて格納した**「総当たり共分散マップ」**と解釈できます。これにより、「$\\boldsymbol{X}$ のどの成分が、$\\boldsymbol{Y}$ のどの成分と、特に強い線形関係にあるか」を一目で把握することが可能になります。\n",
    "\n",
    "***\n",
    "\n",
    "### 第六幕：標本共分散における $1/(n-1)$ の役割\n",
    "\n",
    "「なぜ共分散行列 $S$ の定義に $1/(n-1)$ が含まれているのに、負荷量の共分散を計算する際に再度 $1/(n-1)$ が出てくるのか？」という疑問は、統計学の**「母集団（理論）」と「標本（データ）」の区別**に関わる重要な点です。\n",
    "\n",
    "結論として、矛盾はなく、証明のプロセスはデータ（標本）の世界で完全に整合性が取れています。\n",
    "\n",
    "1.  **理論（母集団）と現実（標本）**:\n",
    "    * **理論の世界**: 共分散は確率変数の「偏差積の期待値」 $E[(X-\\mu_X)(Y-\\mu_Y)]$ として定義されます。\n",
    "    * **現実の世界（データ）**: 真の期待値は未知なため、データから**推定**します。そのための計算式が**標本共分散** $\\frac{1}{n-1}\\sum (x_i-\\bar{x})(y_i-\\bar{y})$ です。\n",
    "\n",
    "2.  **証明の再訪：標本の言葉で語る**\n",
    "    第四幕の証明は、「もう一度割っている」わけではありません。\n",
    "    * 証明の出発点である $\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\frac{1}{n-1}\\boldsymbol{x}_k^T \\boldsymbol{y}_j$ は、**標本共分散という操作の定義そのもの**です。\n",
    "    * 証明の途中で出現する $\\left( \\frac{1}{n-1} X_c^T X_c \\right)$ は、**標本分散共分散行列 $S$ という行列の定義そのもの**です。\n",
    "\n",
    "この証明が示しているのは、**「標本共分散の定義」と「標本分散共分散行列の定義」が数学的に見事に整合しており、$\\text{Cov}(X_c, \\boldsymbol{y}_j) = S \\boldsymbol{w}_j = \\lambda_j \\boldsymbol{w}_j$ という美しい関係式を導くこと**なのです。\n",
    "\n",
    "***\n",
    "\n",
    "### 結論：最適化が暴くデータの構造\n",
    "\n",
    "以上をまとめると、主成分分析の核心的論理は以下のようになります。\n",
    "\n",
    "1.  統計的な要求である**「射影分散の最大化」**は、数学的には**「二次形式の最大化」**という最適化問題として厳密に定式化される。\n",
    "2.  この制約付き最適化問題は、**ラグランジュの未定乗数法**という数学的ツールによって解かれる。\n",
    "3.  その解を求めるプロセスは、必然的に分散共分散行列の**「固有値問題」**に帰着する。\n",
    "4.  最大の分散を与える方向（第一主成分）は**最大固有値に対応する固有ベクトル**として、次に大きい分散を与える方向（第二主成分）は**二番目に大きい固有値に対応する固有ベクトル**として、一意に定まる。\n",
    "\n",
    "このように、主成分分析とは、最適化という数学的な操作を通じて、データ構造の核心（分散共分散行列の固有性）を明らかにする、極めて論理的で美しいプロセスなのです。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
