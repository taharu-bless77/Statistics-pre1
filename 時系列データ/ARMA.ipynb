{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17969959",
   "metadata": {},
   "source": [
    "承知いたしました。ラグ演算子 $L$ を使った数式変形ですね。\n",
    "ここは、一見すると複雑に見えますが、やっていることは「**$Y_t$ と $\\epsilon_t$ について、それぞれ共通因数でくくる（まとめる）**」という、中学数学で行う因数分解と全く同じ操作です。\n",
    "\n",
    "ステップ・バイ・ステップで、丁寧に解説します。\n",
    "\n",
    "---\n",
    "\n",
    "### ステップ1：元のARMA(p,q)モデルの式\n",
    "\n",
    "まず、スタート地点の式を確認します。\n",
    "\n",
    "$$\n",
    "Y_t = c + \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q}\n",
    "$$\n",
    "\n",
    "### ステップ2：$Y$ の項を左辺に、$c$ と $\\epsilon$ の項を右辺に集める\n",
    "\n",
    "モデルの「**入力（$\\epsilon$）**」と「**出力（$Y$）**」を分けるため、過去の $Y$ の項（AR項）をすべて左辺に**移項**します。\n",
    "\n",
    "$$\n",
    "Y_t - \\phi_1 Y_{t-1} - \\dots - \\phi_p Y_{t-p} = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q}\n",
    "$$\n",
    "\n",
    "### ステップ3：ラグ演算子 $L$ の定義を使って書き換える\n",
    "\n",
    "ここで、ラグ演算子 $L$ の定義を思い出します。\n",
    "* $Y_{t-1} = L Y_t$\n",
    "* $Y_{t-2} = L^2 Y_t$\n",
    "* ...\n",
    "* $Y_{t-p} = L^p Y_t$\n",
    "\n",
    "これと全く同じことを、$\\epsilon$ にも適用します。\n",
    "* $\\epsilon_{t-1} = L \\epsilon_t$\n",
    "* $\\epsilon_{t-2} = L^2 \\epsilon_t$\n",
    "* ...\n",
    "* $\\epsilon_{t-q} = L^q \\epsilon_t$\n",
    "\n",
    "この定義を、ステップ2の式に**代入**します。\n",
    "\n",
    "**【左辺】**\n",
    "$Y_t - \\phi_1 (L Y_t) - \\phi_2 (L^2 Y_t) - \\dots - \\phi_p (L^p Y_t)$\n",
    "\n",
    "**【右辺】**\n",
    "$c + \\epsilon_t + \\theta_1 (L \\epsilon_t) + \\theta_2 (L^2 \\epsilon_t) + \\dots + \\theta_q (L^q \\epsilon_t)$\n",
    "\n",
    "### ステップ4：共通因数 $Y_t$ と $\\epsilon_t$ で「くくる」（因数分解）\n",
    "\n",
    "ここが一番のポイントです。$L$ を $x$ や $a$ のような**普通の文字（変数）**と同じように扱って、共通因数でくくります。\n",
    "\n",
    "**【左辺】**\n",
    "$Y_t$ が全ての項に共通しています。\n",
    "$Y_t - \\phi_1 L Y_t - \\phi_2 L^2 Y_t - \\dots - \\phi_p L^p Y_t$\n",
    "\n",
    "ここで、$Y_t$ は $1 \\cdot Y_t$ と考えるのがミソです。\n",
    "$(1 \\cdot Y_t) - (\\phi_1 L \\cdot Y_t) - (\\phi_2 L^2 \\cdot Y_t) - \\dots - (\\phi_p L^p \\cdot Y_t)$\n",
    "\n",
    "これを $Y_t$ でくくると、\n",
    "$$\n",
    "(1 - \\phi_1 L - \\phi_2 L^2 - \\dots - \\phi_p L^p) Y_t\n",
    "$$\n",
    "となります。\n",
    "\n",
    "**【右辺】**\n",
    "$c$ 以外の項は、$\\epsilon_t$ が全てに共通しています。\n",
    "$c + (1 \\cdot \\epsilon_t) + (\\theta_1 L \\cdot \\epsilon_t) + (\\theta_2 L^2 \\cdot \\epsilon_t) + \\dots + (\\theta_q L^q \\cdot \\epsilon_t)$\n",
    "\n",
    "これを $\\epsilon_t$ でくくると、\n",
    "$$\n",
    "c + (1 + \\theta_1 L + \\theta_2 L^2 + \\dots + \\theta_q L^q) \\epsilon_t\n",
    "$$\n",
    "となります。\n",
    "\n",
    "### ステップ5：合体させて、名前を付ける\n",
    "\n",
    "ステップ4で変形した左辺と右辺を、再びイコールで結びます。\n",
    "\n",
    "$$\n",
    "(1 - \\phi_1 L - \\dots - \\phi_p L^p) Y_t = c + (1 + \\theta_1 L + \\dots + \\theta_q L^q) \\epsilon_t\n",
    "$$\n",
    "\n",
    "これで、ご提示いただいた変形が完了しました。\n",
    "\n",
    "最後に、この $L$ の多項式（$L$ の式）に、それぞれ名前を付けます。\n",
    "\n",
    "* **AR特性多項式:** $\\Phi_p(L) = (1 - \\phi_1 L - \\dots - \\phi_p L^p)$\n",
    "* **MA特性多項式:** $\\Theta_q(L) = (1 + \\theta_1 L + \\dots + \\theta_q L^q)$\n",
    "\n",
    "この名前を使うと、あの複雑だったARMA(p,q)モデルの式が、\n",
    "\n",
    "$$\n",
    "\\Phi_p(L) Y_t = c + \\Theta_q(L) \\epsilon_t\n",
    "$$\n",
    "\n",
    "という、非常にスッキリした形で表現できるわけです。\n",
    "\n",
    "---\n",
    "\n",
    "この変形は、単に見た目をスッキリさせるだけでなく、$\\Phi_p(L)=0$ や $\\Theta_q(L)=0$ という「特性方程式」の解を調べることで、モデルの「定常性」や「反転可能性」といった数学的な性質を分析するために不可欠な操作となります。\n",
    "\n",
    "次は、いよいよ「**ARIMA(p,d,q)モデル**」について解説します。これは、非定常なデータ（トレンドを持つデータなど）を扱うための、ARMAモデルの強力な拡張です。いかがでしょうか？"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
