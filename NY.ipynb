{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f96bc62",
   "metadata": {},
   "source": [
    "\n",
    "## 📚 各要素の数学的な役割\n",
    "\n",
    "あなたが挙げた6つの要素が、ニューラルネットワークの「計算」と「学習」のすべてを担っています。\n",
    "\n",
    "### 1. 入力 ($x$)\n",
    "* **役割**: 説明変数、特徴量ベクトル。\n",
    "* **数学的実体**: $\\mathbb{R}^n$ のベクトル。記事の例では、アヤメの4つの特徴量（がく片の長さ等）なので、入力は4次元ベクトル ($n=4$) です。\n",
    "    * $x = (x_1, x_2, x_3, x_4)$\n",
    "\n",
    "### 2. 重み ($w$) と 3. バイアス ($b$)\n",
    "* **役割**: 調整すべき**パラメータ**。\n",
    "* **数学的実体**:\n",
    "    * **重み ($w$)**: 入力 $x$ の各要素に掛ける係数。入力ベクトルの各要素の「重要度」を調整します。\n",
    "    * **バイアス ($b$)**: 重み付き和に加算する「切片」や「下駄」。ノードがどれだけ「発火しやすいか」を調整します。\n",
    "* **学習の目的**: 損失関数 $L$ を最小にするような $w$ と $b$ の最適な組み合わせを見つけること。\n",
    "\n",
    "### 4. 線形和関数 ($y = w \\cdot x + b$)\n",
    "* **役割**: 入力 $x$ を**線形変換**（アフィン変換）する。\n",
    "* **数学的実体 (写像として)**:\n",
    "    * **入力**: 前の層の出力ベクトル $x \\in \\mathbb{R}^n$\n",
    "    * **出力**: 活性化関数への入力値 $y \\in \\mathbb{R}$ （1ノードの場合）\n",
    "    * **写像**: これは、$\\mathbb{R}^n \\to \\mathbb{R}$ への**アフィン写像**（線形変換 $w \\cdot x$ ＋ 平行移動 $+b$）です。\n",
    "    * 記事のように層全体（入力4次元→隠れ層2次元）で考えると、$W_1$ は $(4 \\times 2)$ の行列、$B_1$ は $(1 \\times 2)$ のベクトル（または行列）となり、$XW_1 + B_1$ によって $\\mathbb{R}^4 \\to \\mathbb{R}^2$ へのアフィン写像を行います。\n",
    "\n",
    "### 5. 活性化関数 ($a = f(y)$)\n",
    "* **役割**: 線形和 $y$ を**非線形変換**する。\n",
    "* **数学的実体 (写像として)**:\n",
    "    * **入力**: 線形和関数の出力 $y \\in \\mathbb{R}$\n",
    "    * **出力**: このノードの最終出力 $a$\n",
    "    * **写像**: $\\mathbb{R} \\to \\mathbb{R}$ への**非線形写像**。記事の例ではシグモイド関数 $f(y) = \\frac{1}{1 + e^{-y}}$ です。出力は (0, 1) の範囲に収まります。\n",
    "\n",
    "### 6. 損失関数 ($L$)\n",
    "* **役割**: ネットワークの「学習のゴール」を定義する。\n",
    "* **数学的実体**:\n",
    "    * **入力**: ネットワークの最終予測値 $A$ と、教師データ（正解） $\\text{Truth}$。\n",
    "    * **出力**: 2つのズレを表す実数値（スカラー） $L \\in \\mathbb{R}$。\n",
    "    * 記事の例では**平均二乗誤差** $L = \\frac{(A - \\text{Truth})^2}{2}$ が使われています。\n",
    "    * $L$ は $A$ の関数ですが、その $A$ は $w$ と $b$ によって決まるため、$L$ は最終的に **$w$ と $b$ の関数 $L(w, b)$** とみなせます。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 線形モデルとの決定的な違い\n",
    "\n",
    "ここが最も重要です。\n",
    "\n",
    "### 線形モデル（例: ロジスティック回帰）\n",
    "ロジスティック回帰の数式を見てみましょう。\n",
    "\n",
    "$$\n",
    "p = \\text{sigmoid}(w \\cdot x + b)\n",
    "$$\n",
    "\n",
    "これは、**「線形和関数 $\\to$ 活性化関数 $\\to$ 出力」** という1ステップの処理です。\n",
    "これはニューラルネットワークの「1層（ノード1個）」と全く同じ構造です。\n",
    "\n",
    "### ニューラルネットワーク（多層パーセプトロン）\n",
    "ニューラルネットワークは、この構造を**積み重ねます**。\n",
    "\n",
    "* **隠れ層**: $h = \\text{sigmoid}(W_1 \\cdot x + B_1)$\n",
    "* **出力層**: $A = \\text{sigmoid}(W_2 \\cdot h + B_2)$\n",
    "\n",
    "違いは、**活性化関数（非線形写像）を通過した出力 $h$ を、さらに次の層の入力として使っている**点です。\n",
    "\n",
    "**「なぜそうするのか（意図）」：**\n",
    "もし活性化関数がなければ（または線形関数なら）、層をいくら重ねても、数学的には $W_2 \\cdot (W_1 \\cdot x + B_1) + B_2 = (W_2 W_1)x + (W_2 B_1 + B_2)$ となり、結局は**ただの1回の線形変換**（$W'x + B'$）と同じことになってしまいます。\n",
    "\n",
    "\n",
    "\n",
    "間に**非線形**な活性化関数を挟むことで、**「空間をグニャリと歪ませる」** ことが可能になります。\n",
    "線形モデルがデータ空間に「直線」や「平面」を引くことしかできないのに対し、ニューラルネットワークは活性化関数という「歪み」を何度も重ねることで、非常に複雑な境界線を描くことができます。\n",
    "\n",
    "\n",
    "\n",
    "この**「非線形変換の積み重ね」**による表現力の高さこそが、ニューラルネットワークが他の線形モデルと根本的に異なる点です。\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 逆伝播へのヒント\n",
    "\n",
    "順伝播が「入力 $x$ から $w, b$ を使って $L$ を計算する」プロセスだったのに対し、\n",
    "逆伝播は「**$L$ を $w$ や $b$ で偏微分した値 $\\frac{\\partial L}{\\partial w}$ を計算する**」プロセスです。\n",
    "\n",
    "ステップ8で計算している $\\frac{\\partial L}{\\partial y}$ は、そのための「部品」です。\n",
    "$L$ は $w$ の直接の関数ではなく、$L \\to a \\to y \\to w$ という連鎖になっているため、連鎖律（Chain Rule）を使って逆から計算しているのです。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
