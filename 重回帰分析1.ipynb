{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29dd5a9a",
   "metadata": {},
   "source": [
    "はい、承知いたしました。\n",
    "それでは、前回の回答をベースに、行列初心者の方でも式の展開やベクトル微分を追えるよう、**基礎の基礎から**解説を加えた完全版の記事を作成します。\n",
    "\n",
    "---\n",
    "\n",
    "### ## 📖 行列で学ぶ重回帰分析：ゼロからわかる数学的背景\n",
    "\n",
    "この記事では、重回帰分析の裏側にある数学を、行列計算をベースにゼロから徹底的に解説します。決定係数($R^2$)やF検定といった指標がどのように計算されるのか、そしてなぜ正則化が必要になるのかを、一歩ずつ丁寧に見ていきましょう。\n",
    "\n",
    "#### **前提：すべての基本となる「記号」の定義**\n",
    "\n",
    "まず、登場人物となるベクトルと行列を定義します。$n$個のデータ（サンプル）と$k$個の説明変数がある状況を考えます。\n",
    "\n",
    "* **$y$ (目的変数ベクトル)**: 予測したい実際の値のリストです。($n \\times 1$ の縦長ベクトル)\n",
    "    $$y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$$\n",
    "* **$X$ (計画行列)**: 説明変数のデータ行列です。切片（定数項）を計算するため、最初の列にはすべて1を入れます。($n \\times (k+1)$ の行列)\n",
    "    $$X = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{pmatrix}$$\n",
    "* **$\\beta$ (回帰係数ベクトル)**: 私たちが求めたい「重み」のリストです。($(k+1) \\times 1$ の縦長ベクトル)\n",
    "    $$\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{pmatrix}$$\n",
    "* **$\\hat{y}$ (予測値ベクトル)**: モデルが算出した予測値のリストです。行列の積で $\\hat{y} = X\\hat{\\beta}$ と計算されます。\n",
    "    $$\\hat{y} = \\begin{pmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{pmatrix} = \\begin{pmatrix} \\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 x_{11} + \\dots \\\\ \\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 x_{21} + \\dots \\\\ \\vdots \\\\ \\hat{\\beta}_0 \\cdot 1 + \\hat{\\beta}_1 x_{n1} + \\dots \\end{pmatrix}$$\n",
    "\n",
    "---\n",
    "\n",
    "### ## 1. 💡 ベクトル微分の基礎知識\n",
    "\n",
    "後の正則化の導出で、数式をベクトル $\\beta$ で微分する場面が登場します。そのための準備をしましょう。\n",
    "\n",
    "#### **1.1 なぜベクトルで微分するのか？**\n",
    "目的関数（最小化したい誤差の式）は、$\\beta_0, \\beta_1, \\dots, \\beta_k$ という多数の変数を持つ関数です。この関数の最小値を見つけるには、各変数で偏微分して、そのすべてが0になる点を探す必要があります。\n",
    "ベクトルでの微分（勾配計算）は、これら多数の偏微分を一度に、そして綺麗に表現するための強力なツールです。\n",
    "\n",
    "#### **1.2 対称行列とは？ なぜ $X^T X$ は対称行列なのか？**\n",
    "**対称行列**とは、転置しても形が変わらない行列のことです（$A = A^T$）。例えば、左上から右下への対角線を軸にして、要素が鏡映しになっています。\n",
    "$$\\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$$\n",
    "$(X^T X)$ が対称行列であることは、以下の式変形で簡単に確認できます。\n",
    "$$(X^T X)^T = X^T (X^T)^T = X^T X$$\n",
    "行列が自身の転置と等しいので、$(X^T X)$ は常に対称行列です。これは後の微分公式を適用するための重要な性質です。\n",
    "\n",
    "#### **1.3 微分公式の導出**\n",
    "* **公式1**: $\\frac{\\partial(\\mathbf{a}^T\\mathbf{x})}{\\partial\\mathbf{x}} = \\mathbf{a}$\n",
    "    * **導出**: $\\mathbf{a}^T\\mathbf{x}$ は内積なので、具体的に書くと $a_1 x_1 + a_2 x_2 + \\dots + a_k x_k$ です。\n",
    "    * これを一つの要素 $x_i$ で偏微分すると、$\\frac{\\partial}{\\partial x_i}(a_1 x_1 + \\dots + a_i x_i + \\dots) = a_i$ となります。\n",
    "    * ベクトルでの微分（勾配）は、これらの偏微分結果をベクトルにまとめたものなので、\n",
    "        $$\\frac{\\partial(\\mathbf{a}^T\\mathbf{x})}{\\partial\\mathbf{x}} = \\begin{pmatrix} \\partial/\\partial x_1 \\\\ \\vdots \\\\ \\partial/\\partial x_k \\end{pmatrix} (\\mathbf{a}^T\\mathbf{x}) = \\begin{pmatrix} a_1 \\\\ \\vdots \\\\ a_k \\end{pmatrix} = \\mathbf{a}$$\n",
    "* **公式2**: $\\frac{\\partial(\\mathbf{x}^T\\mathbf{A}\\mathbf{x})}{\\partial\\mathbf{x}} = 2\\mathbf{A}\\mathbf{x}$ （ただしAは対称行列）\n",
    "    * **導出 (2x2の例)**: $\\mathbf{x}^T\\mathbf{A}\\mathbf{x} = a_{11}x_1^2 + 2a_{12}x_1x_2 + a_{22}x_2^2$ （$a_{12}=a_{21}$ を利用）\n",
    "    * $x_1$ で偏微分: $2a_{11}x_1 + 2a_{12}x_2$\n",
    "    * $x_2$ で偏微分: $2a_{12}x_1 + 2a_{22}x_2$\n",
    "    * この結果をベクトルにまとめると $\\begin{pmatrix} 2(a_{11}x_1 + a_{12}x_2) \\\\ 2(a_{12}x_1 + a_{22}x_2) \\end{pmatrix}$ となり、これは $2\\mathbf{A}\\mathbf{x}$ の計算結果と一致します。\n",
    "\n",
    "---\n",
    "\n",
    "### ## 2. 変動の分解：3つの平方和\n",
    "\n",
    "モデルの性能を評価する指標はすべて、目的変数 $y$ の**全変動 (SST)** を、**モデルで説明できた変動 (SSR)** と**説明できなかった変動 (SSE)** に分解することから始まります。\n",
    "\n",
    "#### **SSE (残差平方和)**\n",
    "モデルの予測がどれだけ外れたか、つまり「誤差」の大きさを表します。\n",
    "$$SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\mathbf{e^T e = (y - \\hat{y})^T (y - \\hat{y})}$$\n",
    "\n",
    "#### **SST (全変動)**\n",
    "$y$ 自体が、その平均 $\\bar{y}$ からどれだけばらついているかを示します。\n",
    "$$SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = (y - \\mathbf{1}\\bar{y})^T (y - \\mathbf{1}\\bar{y})$$\n",
    "* **途中式**:\n",
    "    $$SST = (y^T - \\bar{y}\\mathbf{1}^T) (y - \\mathbf{1}\\bar{y}) = y^T y - y^T(\\mathbf{1}\\bar{y}) - (\\bar{y}\\mathbf{1}^T)y + (\\bar{y}\\mathbf{1}^T)(\\mathbf{1}\\bar{y})$$\n",
    "    ここで、$y^T \\mathbf{1} = \\sum y_i = n\\bar{y}$ であり、$\\mathbf{1}^T \\mathbf{1} = n$ なので、\n",
    "    $$SST = y^T y - (n\\bar{y})\\bar{y} - \\bar{y}(n\\bar{y}) + \\bar{y}(n)\\bar{y} = y^T y - n\\bar{y}^2 - n\\bar{y}^2 + n\\bar{y}^2$$   $$\\mathbf{SST = y^T y - n\\bar{y}^2}$$\n",
    "\n",
    "#### **SSR (回帰変動)**\n",
    "モデルの予測値 $\\hat{y}$ が、平均 $\\bar{y}$ からどれだけばらついているかを示し、モデルが**説明できた**変動量を表します。\n",
    "$$\\mathbf{SSR = \\hat{y}^T \\hat{y} - n\\bar{y}^2}$$\n",
    "（SSTの導出で $y$ を $\\hat{y}$ に置き換えることで同様に導けます）\n",
    "\n",
    "---\n",
    "\n",
    "### ## 3. 決定係数 $R^2$ と自由度調整済み決定係数\n",
    "\n",
    "#### **決定係数 ($R^2$)**\n",
    "全変動のうち、モデルが説明できた変動の割合です。\n",
    "$$R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = \\mathbf{1 - \\frac{e^T e}{y^T y - n\\bar{y}^2}}$$\n",
    "\n",
    "#### **自由度調整済み決定係数 ($R^2_{adj}$)**\n",
    "変数の数($k$)を考慮してペナルティを課すことで、$R^2$の欠点を補正します。\n",
    "$$R^2_{adj} = 1 - \\frac{SSE / (n - k - 1)}{SST / (n - 1)} = \\mathbf{1 - \\frac{(e^T e) / (n - k - 1)}{(y^T y - n\\bar{y}^2) / (n - 1)}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### ## 4. モデル全体の有意性検定（F検定）\n",
    "\n",
    "モデル全体が予測に役立つかを検定します。F統計量は「説明できた変動」と「できなかった変動」の比率です。\n",
    "$$F = \\frac{MSR}{MSE} = \\frac{SSR / k}{SSE / (n - k - 1)}$$\n",
    "* **行列での表現**:\n",
    "    $$F = \\mathbf{\\frac{(\\hat{y}^T \\hat{y} - n\\bar{y}^2) / k}{(e^T e) / (n - k - 1)}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### ## 5. 🧠 正則化の導出（詳細版）\n",
    "\n",
    "#### **Ridge回帰 (L2正則化)**\n",
    "OLSの目的関数に、係数の大きさを抑えるペナルティ項を加えます。\n",
    "* **目的関数**: $J(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda \\beta^T \\beta$\n",
    "* **導出（微分）**: この関数を $\\beta$ で微分して0とおきます。\n",
    "    1.  **展開**:\n",
    "        $$J(\\beta) = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda \\beta^T \\beta$$\n",
    "    2.  **各項を微分**: 先ほど導出したベクトル微分の公式を適用します。\n",
    "        * `定数項` $y^T y$ $\\rightarrow$ $0$\n",
    "        * $-2\\beta^T (X^T y)$ $\\rightarrow$ $-2X^T y$ （公式1を適用）\n",
    "        * $\\beta^T (X^T X) \\beta$ $\\rightarrow$ $2(X^T X)\\beta$ （公式2を適用, $A=X^TX$）\n",
    "        * $\\lambda \\beta^T \\beta$ $\\rightarrow$ $2\\lambda I \\beta$ （公式2を適用, $A=\\lambda I$）\n",
    "    3.  **合計して0とおく**:\n",
    "        $$-2X^T y + 2(X^T X)\\beta + 2\\lambda I\\beta = 0$$\n",
    "    4.  **$\\beta$ について解く**:\n",
    "        $$2(X^T X + \\lambda I)\\beta = 2X^T y$$       $$(X^T X + \\lambda I)\\beta = X^T y$$\n",
    "* **解**:\n",
    "    $$\\hat{\\beta}_{Ridge} = (X^T X + \\lambda I)^{-1} X^T y$$\n",
    "\n",
    "#### **Lasso回帰 (L1正則化)**\n",
    "ペナルティ項に係数の絶対値の和を用います。\n",
    "* **目的関数**:\n",
    "    $$\\min_{\\beta} \\left( (y - X\\beta)^T(y - X\\beta) + \\lambda \\sum_{j=1}^{k} |\\beta_j| \\right)$$\n",
    "* **解**: 絶対値の項は微分ができないため、Ridgeのような綺麗な解の公式は存在せず、コンピュータによる数値計算で求められます。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
