{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa7a24e",
   "metadata": {},
   "source": [
    "https://qiita.com/AnchorBlues/items/8fe2483a3a72676eb96d\n",
    "\n",
    "これはベクトル解析その１"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59dc03",
   "metadata": {},
   "source": [
    "https://tenbin.ai/workspace/share/236514983982976000\n",
    "これは、上のurLを元にgeminiが作成した偏回帰係数の導出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca4e7b",
   "metadata": {},
   "source": [
    "はい、承知いたしました。新しく提供されたソース「ベクトルで微分・行列で微分」公式まとめ と、これまでの会話履歴を踏まえ、重回帰分析のモデル設定から始まり、最小二乗法の定式化、そしてソースに記載されているベクトル微分を用いて解を導出する過程を、線形代数の概念に注意しながら、過程を省くことなく説明します。また、ご要望に応じて、ベクトル微分の導出イメージを2次元で補足します。少々長くなりますが、ご了承ください。\n",
    "\n",
    "### 1. 重回帰分析モデルの線形代数による表現\n",
    "\n",
    "まず、**重回帰分析**では、複数の説明変数から目的変数を予測するモデルを考えます。これは、**線形代数**を用いると非常に簡潔に表現できます。\n",
    "\n",
    "$n$個のデータサンプルがあり、それぞれが $p$個の説明変数を持つとします。目的変数を $y$、説明変数を $x_1, x_2, \\dots, x_p$ としたとき、線形回帰モデルは以下のようになります。\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_i$\n",
    "\n",
    "ここで、$i$ はデータサンプルのインデックス（$i=1, \\dots, n$）です。$\\beta_0, \\beta_1, \\dots, \\beta_p$ はモデルの係数（回帰係数）であり、$\\epsilon_i$ は各サンプルにおける誤差（または残差）です。\n",
    "\n",
    "この $n$個の式全体を、**ベクトル**と**行列**を用いて表現します。\n",
    "\n",
    "*   目的変数**ベクトル**: $n$個の目的変数の値を並べた**ベクトル** $\\boldsymbol{y} \\in \\mathbb{R}^n$\n",
    "    $\\boldsymbol{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$\n",
    "\n",
    "*   説明変数**行列** (計画行列 または デザイン行列): 各サンプルの説明変数に、切片項 $\\beta_0$ に対応する1を加えて並べた**行列** $X \\in \\mathbb{R}^{n \\times (p+1)}$\n",
    "    $X = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\dots & x_{np} \\end{pmatrix}$\n",
    "\n",
    "*   係数**ベクトル**: $p+1$個の回帰係数を並べた**ベクトル** $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}$\n",
    "    $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}$\n",
    "\n",
    "*   誤差**ベクトル**: $n$個の誤差を並べた**ベクトル** $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$\n",
    "    $\\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}$\n",
    "\n",
    "これらの**ベクトル**と**行列**を用いると、重回帰モデルは以下の非常に簡潔な**線形代数**の形式で表されます。\n",
    "\n",
    "$\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "\n",
    "私たちの目標は、与えられたデータ（$X$と$\\boldsymbol{y}$）から、未知の係数**ベクトル** $\\boldsymbol{\\beta}$ を推定することです。\n",
    "\n",
    "### 2. 最小二乗法の目的関数\n",
    "\n",
    "重回帰分析における係数**ベクトル** $\\boldsymbol{\\beta}$ の推定は、通常、「**最小二乗法**」という基準に基づいて行われます。最小二乗法の考え方は、「観測された目的変数**ベクトル** $\\boldsymbol{y}$ と、モデルによって予測される値 $X\\boldsymbol{\\beta}$ との間の誤差**ベクトル** $\\boldsymbol{\\epsilon}$ の**長さ（ノルム）**の2乗を最小にする」というものです。\n",
    "\n",
    "誤差**ベクトル** $\\boldsymbol{\\epsilon}$ は、モデル式から $\\boldsymbol{\\epsilon} = \\boldsymbol{y} - X\\boldsymbol{\\beta}$ となります。\n",
    "誤差**ベクトル**の**長さ**（**ユークリッドノルム**）の2乗は、その**ベクトル**と自身の**転置**との積として定義されます。\n",
    "$||\\boldsymbol{\\epsilon}||^2 = \\boldsymbol{\\epsilon}^T \\boldsymbol{\\epsilon}$\n",
    "\n",
    "したがって、最小二乗法の目的関数は、誤差**ベクトル**のノルムの2乗、つまり二乗誤差の合計を最小化することになります。これを**スカラー**関数 $L(\\boldsymbol{\\beta})$ として定義します。\n",
    "\n",
    "$L(\\boldsymbol{\\beta}) = ||\\boldsymbol{y} - X\\boldsymbol{\\beta}||^2 = (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T (\\boldsymbol{y} - X\\boldsymbol{\\beta})$\n",
    "\n",
    "この**スカラー**関数 $L(\\boldsymbol{\\beta})$ は、係数**ベクトル** $\\boldsymbol{\\beta}$ の各要素 $\\beta_0, \\beta_1, \\dots, \\beta_p$ に依存する関数です。最小二乗法の目標は、この関数 $L(\\boldsymbol{\\beta})$ の値を最小にする**ベクトル** $\\boldsymbol{\\beta}$ を見つけることです。\n",
    "\n",
    "### 3. ベクトル微分による目的関数の最小化\n",
    "\n",
    "**スカラー**関数を最小化するには、その関数の勾配（微分した結果の**ベクトル**）を計算し、それがゼロ**ベクトル**になる点を見つけます。これは、多変数関数の最小化における一般的な手法であり、**線形代数**における**ベクトル**微分がそのツールとなります。\n",
    "\n",
    "最小二乗法の目的関数 $L(\\boldsymbol{\\beta}) = (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T (\\boldsymbol{y} - X\\boldsymbol{\\beta})$ は、ソース に記載されている**スカラーのベクトル微分**公式の形式によく似ています。ソースの公式は $\\frac{ \\partial }{ \\partial \\boldsymbol{x} } \\left(A\\boldsymbol{x} - \\boldsymbol{b} \\right)^T \\left(A\\boldsymbol{x} - \\boldsymbol{b} \\right) = 2A^T \\left( A \\boldsymbol{x} -\\boldsymbol{b} \\right)$ です。\n",
    "\n",
    "ここで、ソースの公式の**ベクトル** $\\boldsymbol{x}$ を私たちの係数**ベクトル** $\\boldsymbol{\\beta}$ に、**行列** $A$ を計画**行列** $X$ に、**ベクトル** $\\boldsymbol{b}$ を目的変数**ベクトル** $\\boldsymbol{y}$ に対応させます。\n",
    "\n",
    "すると、目的関数 $L(\\boldsymbol{\\beta})$ を**ベクトル** $\\boldsymbol{\\beta}$ で微分する計算は、ソース の公式を用いて以下のように実行できます。\n",
    "\n",
    "$\\frac{ \\partial L }{ \\partial \\boldsymbol{\\beta} } = \\frac{ \\partial }{ \\partial \\boldsymbol{\\beta} } (\\boldsymbol{y} - X\\boldsymbol{\\beta})^T (\\boldsymbol{y} - X\\boldsymbol{\\beta})$\n",
    "\n",
    "ここで、$(\\boldsymbol{y} - X\\boldsymbol{\\beta})^T (\\boldsymbol{y} - X\\boldsymbol{\\beta}) = (X\\boldsymbol{\\beta} - \\boldsymbol{y})^T (X\\boldsymbol{\\beta} - \\boldsymbol{y})$ であることを利用すると、ソースの公式を直接適用できます。\n",
    "\n",
    "$\\frac{ \\partial }{ \\partial \\boldsymbol{\\beta} } (X\\boldsymbol{\\beta} - \\boldsymbol{y})^T (X\\boldsymbol{\\beta} - \\boldsymbol{y}) = 2X^T (X\\boldsymbol{\\beta} - \\boldsymbol{y})$\n",
    "\n",
    "最小値を求めるためには、この勾配**ベクトル**をゼロ**ベクトル**に等しいとおきます。\n",
    "\n",
    "$2X^T (X\\boldsymbol{\\beta} - \\boldsymbol{y}) = \\boldsymbol{0}$\n",
    "\n",
    "両辺を1/2倍しても式は変わらないので、\n",
    "\n",
    "$X^T (X\\boldsymbol{\\beta} - \\boldsymbol{y}) = \\boldsymbol{0}$\n",
    "\n",
    "**行列**の分配法則と**転置**の性質（$(AB)^T = B^T A^T$）を使って展開します。\n",
    "\n",
    "$X^T X \\boldsymbol{\\beta} - X^T \\boldsymbol{y} = \\boldsymbol{0}$\n",
    "\n",
    "$X^T X \\boldsymbol{\\beta} = X^T \\boldsymbol{y}$\n",
    "\n",
    "この**線形**方程式は、**最小二乗法**において非常に重要な**正規方程式**と呼ばれます（正規方程式という用語はソースには直接記載されていませんが、ソースの公式 を用いて導出された結果です）。これは、未知の**ベクトル** $\\boldsymbol{\\beta}$ に関する**線形**方程式系であり、**線形代数**の手法を用いて解くことができます。\n",
    "\n",
    "### 4. 正規方程式の解法（線形代数による）\n",
    "\n",
    "導出された**正規方程式** $X^T X \\boldsymbol{\\beta} = X^T \\boldsymbol{y}$ を解くことで、最小二乗解 $\\hat{\\boldsymbol{\\beta}}$ を得ます。\n",
    "\n",
    "$X^T X$ は $(p+1) \\times (p+1)$ の**正方行列**です。もしこの**行列** $X^T X$ が**正則**（**逆行列**を持つ）であれば、その**逆行列** $(X^T X)^{-1}$ を正規方程式の左から掛けることで、解を解析的に求めることができます。\n",
    "\n",
    "$(X^T X)^{-1} (X^T X \\boldsymbol{\\beta}) = (X^T X)^{-1} (X^T \\boldsymbol{y})$\n",
    "\n",
    "$((X^T X)^{-1} X^T X) \\boldsymbol{\\beta} = (X^T X)^{-1} X^T \\boldsymbol{y}$\n",
    "\n",
    "$I \\boldsymbol{\\beta} = (X^T X)^{-1} X^T \\boldsymbol{y}$ （ここで $I$ は**単位行列**）\n",
    "\n",
    "$\\hat{\\boldsymbol{\\beta}} = (X^T X)^{-1} X^T \\boldsymbol{y}$\n",
    "\n",
    "これが、**最小二乗法**による回帰係数**ベクトル** $\\boldsymbol{\\beta}$ の推定値 $\\hat{\\boldsymbol{\\beta}}$ を求める公式です。この解は、**行列**の**積**、**転置**、そして**逆行列**といった、純粋な**線形代数**の演算によって得られます。$(X^T X)^{-1} X^T$ の部分は、**擬似逆行列**と呼ばれる**線形代数**の概念に密接に関連しています（擬似逆行列という用語もソースには記載されていません）。\n",
    "\n",
    "このように、重回帰モデルから出発し、二乗誤差を**線形代数**的な目的関数として定義し、ソース に示されているような**ベクトル**微分公式を用いてこの目的関数を微分し、勾配をゼロとおくことで得られる**線形**方程式系（正規方程式）を**線形代数**的に解く、というのが**最小二乗法**のアルゴリズムの核心部分です。ソース は、この過程で不可欠となる**ベクトル**や**行列**の微分公式を提供しており、**機械学習**などの分野で広く応用される**線形代数**の強力なツールであることが示されています。\n",
    "\n",
    "### 5. ベクトル微分の導出補足（2次元の例）\n",
    "\n",
    "ソース では、**スカラー** $f$ を**ベクトル** $\\boldsymbol{x} \\in \\mathbb{R}^n$ で微分する定義が $\\frac{ \\partial f }{ \\partial \\boldsymbol{x} } = \\left( \\frac{ \\partial f }{ \\partial {x}_{1} }, \\dots, \\frac{ \\partial f }{ \\partial {x}_{n} } \\right)^T \\in \\mathbb{R}^n$ であると示されています。これは、**スカラー**関数をその**ベクトル**の各要素で偏微分した結果を縦に並べた**ベクトル**（勾配**ベクトル**）であるという定義です。\n",
    "\n",
    "この定義 に基づいて、ソース にある簡単な公式がどのように導出されるかを2次元の例で見てみます。$\\boldsymbol{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ とします。\n",
    "\n",
    "**例1: $\\frac{ \\partial }{ \\partial \\boldsymbol{x} } \\boldsymbol{x}^T\\boldsymbol{x} = 2\\boldsymbol{x}$ の導出イメージ**\n",
    "\n",
    "関数 $f(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{x}$ を考えます。これは、$\\boldsymbol{x}^T\\boldsymbol{x} = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1^2 + x_2^2$ という**スカラー**関数です。\n",
    "\n",
    "ソース の定義に従い、この関数を**ベクトル** $\\boldsymbol{x}$ で微分します。これは、各要素 $x_1, x_2$ で偏微分した結果を**ベクトル**として並べるということです。\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (x_1^2 + x_2^2) = 2x_1$\n",
    "$\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (x_1^2 + x_2^2) = 2x_2$\n",
    "\n",
    "したがって、定義 による**ベクトル**微分は、これらの偏微分結果を縦に並べた**ベクトル**になります。\n",
    "\n",
    "$\\frac{ \\partial f }{ \\partial \\boldsymbol{x} } = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix} = 2 \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 2\\boldsymbol{x}$\n",
    "\n",
    "これはソース の公式と一致します。\n",
    "\n",
    "**例2: $\\frac{ \\partial }{ \\partial \\boldsymbol{x} } \\left(\\boldsymbol{x} - \\boldsymbol{a} \\right)^T \\left(\\boldsymbol{x} - \\boldsymbol{a} \\right) = 2\\left(\\boldsymbol{x} - \\boldsymbol{a} \\right)$ の導出イメージ**\n",
    "\n",
    "次に、関数 $g(\\boldsymbol{x}) = (\\boldsymbol{x} - \\boldsymbol{a})^T (\\boldsymbol{x} - \\boldsymbol{a})$ を考えます。ここで $\\boldsymbol{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix}$ は定数**ベクトル**です。\n",
    "$g(\\boldsymbol{x}) = \\begin{pmatrix} x_1 - a_1 & x_2 - a_2 \\end{pmatrix} \\begin{pmatrix} x_1 - a_1 \\\\ x_2 - a_2 \\end{pmatrix} = (x_1 - a_1)^2 + (x_2 - a_2)^2$\n",
    "\n",
    "同様に、各要素 $x_1, x_2$ で偏微分します。\n",
    "\n",
    "$\\frac{\\partial g}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} ((x_1 - a_1)^2 + (x_2 - a_2)^2) = 2(x_1 - a_1)$\n",
    "$\\frac{\\partial g}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} ((x_1 - a_1)^2 + (x_2 - a_2)^2) = 2(x_2 - a_2)$\n",
    "\n",
    "定義 による**ベクトル**微分は、\n",
    "\n",
    "$\\frac{ \\partial g }{ \\partial \\boldsymbol{x} } = \\begin{pmatrix} \\frac{\\partial g}{\\partial x_1} \\\\ \\frac{\\partial g}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2(x_1 - a_1) \\\\ 2(x_2 - a_2) \\end{pmatrix} = 2 \\begin{pmatrix} x_1 - a_1 \\\\ x_2 - a_2 \\end{pmatrix} = 2(\\boldsymbol{x} - \\boldsymbol{a})$\n",
    "\n",
    "これもソース の公式と一致します。\n",
    "\n",
    "最小二乗法で用いる公式 $\\frac{ \\partial }{ \\partial \\boldsymbol{x} } \\left(A\\boldsymbol{x} - \\boldsymbol{b} \\right)^T \\left(A\\boldsymbol{x} - \\boldsymbol{b} \\right) = 2A^T \\left( A \\boldsymbol{x} -\\boldsymbol{b} \\right)$ は、これらの簡単な例と同様に、定義 に基づいて各要素で偏微分し、それを**ベクトル**としてまとめることで導出されますが、**行列** $A$ が関わるため、連鎖律（チェーンルール）など、より複雑な**線形代数**的な微分の規則が必要となります。ソース はその結果公式を提供しており、最小二乗法の解析に直接適用できる形になっています。\n",
    "\n",
    "このように、**ベクトル**微分は、多変数**スカラー**関数の勾配を**ベクトル**として表現する**線形代数**的な概念であり、その定義 に基づいて様々な公式 が導出されます。これらの公式は、最小二乗法のような最適化問題における勾配計算に不可欠なツールとなります。\n",
    "\n",
    "以上が、重回帰分析モデルから始まり、最小二乗法の定式化、そしてソース に記載されている**ベクトル**微分公式を用いた解の導出、さらにその背景にある**ベクトル**微分の概念を2次元で補足した説明となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593dd31",
   "metadata": {},
   "source": [
    "では、重回帰分析における偏回帰係数を用いた簡単な問題を考えてみましょう。\n",
    "\n",
    "### 問題設定\n",
    "\n",
    "ある会社が製品の価格（Y）を予測したいと考えています。彼らは以下の2つの説明変数を使用することにしました：\n",
    "\n",
    "1. **広告費（X1）**: 製品の広告費（単位: 万円）\n",
    "2. **販売促進キャンペーンコスト（X2）**: 販売促進に使ったコスト（単位: 万円）\n",
    "\n",
    "以下のデータが与えられています。\n",
    "\n",
    "| 序号 | 広告費 (X1) | 販売促進 (X2) | 価格 (Y) |\n",
    "|------|--------------|----------------|---------|\n",
    "| 1    | 10           | 20             | 100     |\n",
    "| 2    | 15           | 30             | 150     |\n",
    "| 3    | 20           | 45             | 200     |\n",
    "| 4    | 25           | 50             | 250     |\n",
    "| 5    | 30           | 60             | 300     |\n",
    "\n",
    "### 質問\n",
    "\n",
    "1. 上記のデータを基に、重回帰分析を行い、偏回帰係数を求めてください。\n",
    "2. 偏回帰係数を用いて、広告費が25万円、販売促進が40万円のときの価格の予測値を求めてください。\n",
    "\n",
    "### 解法の手順"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e4427",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "では、重回帰分析における各ステップを詳しく説明します。\n",
    "\n",
    "## 1. 行列式の準備\n",
    "\n",
    "### 目的変数ベクトル $\\boldsymbol{Y}$\n",
    "\n",
    "価格（$Y$）は、以下のようになります。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Y} = \\begin{pmatrix} 100 \\\\ 150 \\\\ 200 \\\\ 250 \\\\ 300 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### 説明変数行列 $\\mathbf{X}$\n",
    "\n",
    "説明変数は広告費（$X_1$）と販売促進キャンペーンコスト（$X_2$）です。この行列には切片項を追加するための列を含めます。\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & 10 & 20 \\\\\n",
    "1 & 15 & 30 \\\\\n",
    "1 & 20 & 45 \\\\\n",
    "1 & 25 & 50 \\\\\n",
    "1 & 30 & 60\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "ここで、最初の列は切片項のための「1」の列です。\n",
    "\n",
    "### 係数ベクトル $\\boldsymbol{\\beta}$\n",
    "\n",
    "係数ベクトルは、回帰係数を含むベクトルです。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### モデル設定\n",
    "\n",
    "回帰モデルの形式は以下のようになります。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "ここで、$\\boldsymbol{\\epsilon}$は誤差項です。\n",
    "\n",
    "## 2. 偏回帰係数ベクトルの計算\n",
    "\n",
    "偏回帰係数ベクトルは次のように計算されます。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{Y}\n",
    "$$\n",
    "\n",
    "### ステップごとの計算\n",
    "\n",
    "1. **転置行列の計算**:\n",
    "    $$\n",
    "    \\mathbf{X}^T = \\begin{pmatrix}\n",
    "    1 & 1 & 1 & 1 & 1 \\\\\n",
    "    10 & 15 & 20 & 25 & 30 \\\\\n",
    "    20 & 30 & 45 & 50 & 60\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "2. **行列の積を計算**:\n",
    "    $$\n",
    "    \\mathbf{X}^T \\mathbf{X} = \\begin{pmatrix}\n",
    "    5 & 100 & 175 \\\\\n",
    "    100 & 2500 & 3750 \\\\\n",
    "    175 & 3750 & 4975\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "3. **逆行列を計算**:\n",
    "    この行列の逆行列 $(\\mathbf{X}^T \\mathbf{X})^{-1}$ を計算します。\n",
    "\n",
    "4. **行列の積を計算**:\n",
    "    $$\n",
    "    \\mathbf{X}^T \\boldsymbol{Y} = \\begin{pmatrix}\n",
    "    1000 \\\\\n",
    "    18500 \\\\\n",
    "    25750\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "5. **最終的な係数ベクトル $\\boldsymbol{\\beta}$ の計算**:\n",
    "    各ステップで得られた結果を利用し、偏回帰係数を求めます。\n",
    "\n",
    "## 3. 予測値の計算\n",
    "\n",
    "求めた係数を用いて価格を予測します。たとえば、広告費が25万円、販売促進が40万円のとき、新しいデータポイントに対する予測は以下のようになります。\n",
    "\n",
    "1. 新しいデータポイントの行列を作成します。切片項に対して1を持たせます。\n",
    "    $$\n",
    "    \\mathbf{X}_{\\text{new}} = \\begin{pmatrix} 1 & 25 & 40 \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "2. 価格の予測を計算します。\n",
    "    $$\n",
    "    \\hat{Y} = \\mathbf{X}_{\\text{new}} \\boldsymbol{\\beta}\n",
    "    $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
