{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、承知いたしました。\n",
    "主成分分析の核心である「射影分散の最大化」と「最適化」の論理について、数式表示を整え、より美しく明瞭な形で再度解説します。\n",
    "\n",
    "### 序章：究極の問い —「最も情報を保持する軸は何か？」\n",
    "\n",
    "3次元空間 $\\mathbb{R}^3$ に散らばる中心化済みデータ点群 $\\{\\boldsymbol{x}_n\\}$ があるとします。このデータの本質を、たった1本の直線（1次元部分空間）で表現したい、というのが我々の最初の要求です。\n",
    "\n",
    "では、「最もよく表現する」とは、数学的にどう定義すればよいでしょうか？ これには2つの同値なアプローチがあります。\n",
    "]\n",
    "1.  **射影誤差の最小化**: 元の点と、直線に射影された点との間の距離（誤差）の合計が最小になる直線。\n",
    "2.  **射影分散の最大化**: 直線に射影された点たちが、最も広範囲に散らばる（分散が最大になる）直線。\n",
    "\n",
    "なぜ**分散の最大化**が重要なのでしょうか？ 分散が大きいということは、それだけ各データ点の特徴（違い）が射影後もよく区別できていることを意味します。情報理論の観点から言えば、**分散は情報量の一つの尺度**であり、分散を最大化することは、射影による**情報損失を最小限に抑える**ことに繋がるからです。\n",
    "\n",
    "ここでは、後者の「射影分散の最大化」という視点から、その論理を徹底的に追っていきます。\n",
    "\n",
    "---\n",
    "\n",
    "### 第一幕：射影分散の厳密な数学的表現\n",
    "\n",
    "**目的：任意の方向ベクトル $\\boldsymbol{a}$ にデータを射影したときの分散 $V(\\boldsymbol{a})$ を、数式で厳密に表現する。**\n",
    "\n",
    "**1. 舞台設定**\n",
    "* 中心化済みデータ点: $\\boldsymbol{x}_n \\in \\mathbb{R}^3$  ($n=1, \\dots, N$)\n",
    "* 探したい直線の方向ベクトル: $\\boldsymbol{a} \\in \\mathbb{R}^3$\n",
    "* 方向ベクトルであるための制約: $||\\boldsymbol{a}||^2 = \\boldsymbol{a}^T\\boldsymbol{a} = 1$\n",
    "\n",
    "**2. 行為1：データ点の射影**\n",
    "あるデータ点 $\\boldsymbol{x}_n$ を、ベクトル $\\boldsymbol{a}$ が定める直線上に正射影します。このとき、原点から射影点までの符号付き距離（座標）は、2つのベクトルの内積で与えられます。\n",
    "\n",
    "$$\\text{score}_n = \\boldsymbol{a}^T\\boldsymbol{x}_n$$\n",
    "\n",
    "これが、データ点 $\\boldsymbol{x}_n$ の、新しい軸 $\\boldsymbol{a}$ 上での値となります。\n",
    "\n",
    "**3. 行為2：射影された点群の分散の計算**\n",
    "この射影スコアの集まり $\\{ \\boldsymbol{a}^T\\boldsymbol{x}_1, \\dots, \\boldsymbol{a}^T\\boldsymbol{x}_N \\}$ の分散 $V(\\boldsymbol{a})$ を計算します。スコアの平均は、データが中心化されているため0になります。\n",
    "\n",
    "$$\\text{E}[\\text{score}] = \\frac{1}{N} \\sum_{n=1}^N (\\boldsymbol{a}^T\\boldsymbol{x}_n) = \\boldsymbol{a}^T \\left( \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n \\right) = \\boldsymbol{a}^T\\boldsymbol{0} = 0$$\n",
    "\n",
    "平均が0の場合、分散は単純に「二乗の平均」となります。\n",
    "\n",
    "$$V(\\boldsymbol{a}) = \\frac{1}{N} \\sum_{n=1}^N (\\text{score}_n)^2 = \\frac{1}{N} \\sum_{n=1}^N (\\boldsymbol{a}^T\\boldsymbol{x}_n)^2$$\n",
    "\n",
    "**4. 行為3：二次形式への変換**\n",
    "この分散の式は、線形代数の力を使うと非常に美しい形に変形できます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(\\boldsymbol{a}) &= \\frac{1}{N} \\sum_{n=1}^N (\\boldsymbol{a}^T\\boldsymbol{x}_n)(\\boldsymbol{a}^T\\boldsymbol{x}_n) \\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{a}^T\\boldsymbol{x}_n\\boldsymbol{x}_n^T\\boldsymbol{a} \\\\\n",
    "&= \\boldsymbol{a}^T \\left( \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n\\boldsymbol{x}_n^T \\right) \\boldsymbol{a}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで、括弧の中の式は、まさしく**分散共分散行列 $\\mathbf{S}$** の定義そのものです。\n",
    "\n",
    "$$\\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n\\boldsymbol{x}_n^T$$\n",
    "\n",
    "したがって、射影分散は以下の**二次形式**で表現されます。\n",
    "\n",
    "$$V(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a}$$\n",
    "\n",
    "**幕間の結論**\n",
    "「最も情報を保持する直線を探す」という曖昧な問題は、\n",
    "**「制約条件 $\\boldsymbol{a}^T\\boldsymbol{a}=1$ の下で、二次形式 $V(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a}$ を最大化する方向ベクトル $\\boldsymbol{a}$ を求めよ」**\n",
    "という、明快かつ厳密な**最適化問題**に完全に翻訳されました。\n",
    "\n",
    "---\n",
    "\n",
    "### 第二幕：最適化問題の解法 —「最良」を見つける数学的道具\n",
    "\n",
    "この最適化問題を解くために、強力な数学的ツールである**ラグランジュの未定乗数法**を用います。これは、制約付きの最適化問題を、制約なしの極値問題へと変換するエレガントな手法です。\n",
    "\n",
    "**1. ラグランジュ関数の構築**\n",
    "目的関数 $f(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a}$ と、制約条件 $g(\\boldsymbol{a}) = \\boldsymbol{a}^T\\boldsymbol{a} - 1 = 0$ を、ラグランジュ乗数 $\\lambda$ を用いて一つの関数にまとめます。\n",
    "\n",
    "$$L(\\boldsymbol{a}, \\lambda) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a} - \\lambda(\\boldsymbol{a}^T\\boldsymbol{a} - 1)$$\n",
    "\n",
    "**2. 極値の探求（勾配=0）**\n",
    "最適解（極値点）では、このラグランジュ関数の勾配がゼロベクトルになります。なぜなら、頂点ではどの方向に微小に動いても関数の値は変化しない（傾きが0）からです。\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{a}} L = \\frac{\\partial L}{\\partial \\boldsymbol{a}} = \\boldsymbol{0}$$\n",
    "\n",
    "ベクトルによる微分の公式を適用すると、\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{a}} L = 2\\mathbf{S}\\boldsymbol{a} - 2\\lambda\\boldsymbol{a} = \\boldsymbol{0}$$\n",
    "\n",
    "これを整理すると、線形代数における最も重要な方程式の一つが得られます。\n",
    "\n",
    "$$\\mathbf{S}\\boldsymbol{a} = \\lambda\\boldsymbol{a}$$\n",
    "\n",
    "**3. 解の解釈**\n",
    "この方程式は、ベクトル $\\boldsymbol{a}$ が分散共分散行列 $\\mathbf{S}$ の**固有ベクトル**であり、スカラー $\\lambda$ が対応する**固有値**でなければならないことを示しています。\n",
    "\n",
    "では、どの固有値・固有ベクトルを選べばよいのでしょうか？\n",
    "このとき、最大化したい分散 $V(\\boldsymbol{a})$ の値は、\n",
    "\n",
    "$$V(\\boldsymbol{a}) = \\boldsymbol{a}^T\\mathbf{S}\\boldsymbol{a} = \\boldsymbol{a}^T(\\lambda\\boldsymbol{a}) = \\lambda(\\boldsymbol{a}^T\\boldsymbol{a}) = \\lambda(1) = \\lambda$$\n",
    "\n",
    "となり、**分散の値が固有値そのものに一致**します。\n",
    "\n",
    "したがって、分散を最大化するためには、**$\\mathbf{S}$ の固有値の中で最大のものを $\\lambda_1$ とし、その時の解となる方向ベクトル $\\boldsymbol{a}$ は、対応する固有ベクトル $\\boldsymbol{e}_1$ である**、と結論付けられます。\n",
    "\n",
    "#### 第二、第三主成分の最適化\n",
    "第二主成分を求める際は、制約に $\\boldsymbol{a}^T\\boldsymbol{e}_1=0$ が加わりますが、最終的には同じ固有値方程式 $\\mathbf{S}\\boldsymbol{a} = \\lambda\\boldsymbol{a}$ に帰着します。そして、直交制約を満たす解の中から最大の分散（固有値）を持つものが選ばれるため、必然的に**2番目に大きい固有値 $\\lambda_2$ とその固有ベクトル $\\boldsymbol{e}_2$** が第二主成分となります。\n",
    "\n",
    "### 結論：最適化が暴くデータの構造\n",
    "\n",
    "以上をまとめると、主成分分析の核心的論理は以下のようになります。\n",
    "\n",
    "1.  統計的な要求である**「射影分散の最大化」**は、数学的には**「二次形式の最大化」**という最適化問題として厳密に定式化される。\n",
    "2.  この制約付き最適化問題は、**ラグランジュの未定乗数法**という数学的ツールによって解かれる。\n",
    "3.  その解を求めるプロセスは、必然的に分散共分散行列の**「固有値問題」**に帰着する。\n",
    "4.  最大の分散を与える方向（第一主成分）は**最大固有値に対応する固有ベクトル**として、次に大きい分散を与える方向（第二主成分）は**二番目に大きい固有値に対応する固有ベクトル**として、一意に定まる。\n",
    "\n",
    "このように、主成分分析とは、最適化という数学的な操作を通じて、データ構造の核心（分散共分散行列の固有性）を明らかにする、極めて論理的で美しいプロセスなのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、素晴らしい質問です。ステップCの「くくり出し」は、多変量解析の数式展開で頻繁に登場する重要な操作です。なぜこれが可能なのか、その理由を2つの側面から詳しく解説します。\n",
    "\n",
    "### 理由1：総和（$\\sum$）の対象と「定数」の役割\n",
    "\n",
    "これが最も本質的な理由です。\n",
    "\n",
    "総和記号 $\\sum_{n=1}^N$ は、**インデックス（添え字）である $n$ が変わると変化する部分だけを足し合わせなさい**、という命令です。\n",
    "\n",
    "我々の式を見てみましょう。\n",
    "$$V(\\boldsymbol{a}) = \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{a}^T\\boldsymbol{x}_n\\boldsymbol{x}_n^T\\boldsymbol{a}$$\n",
    "\n",
    "この式の各項 $\\boldsymbol{a}^T\\boldsymbol{x}_n\\boldsymbol{x}_n^T\\boldsymbol{a}$ の中で、インデックス $n$ が付いているのはデータ点 $\\boldsymbol{x}_n$ だけです。\n",
    "\n",
    "一方、方向ベクトル $\\boldsymbol{a}$ は、**どのデータ点 ($n=1, \\dots, N$) について計算しているかには全く依存しません**。我々が分析の最初に「この方向に射影しよう」と決めた一つのベクトルです。\n",
    "\n",
    "つまり、総和 $\\sum$ の計算において、ベクトル $\\boldsymbol{a}$ とその転置 $\\boldsymbol{a}^T$ は、**定数（定ベクトル）**として扱うことができます。\n",
    "\n",
    "簡単なスカラーの例で考えてみましょう。定数 $c$ と変数 $x_n$ があるとき、\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N c \\cdot x_n \\cdot c = c \\cdot x_1 \\cdot c + c \\cdot x_2 \\cdot c + \\dots + c \\cdot x_N \\cdot c\n",
    "$$\n",
    "\n",
    "これは、共通因数 $c$ を使って、\n",
    "\n",
    "$$\n",
    "c^2 (x_1 + x_2 + \\dots + x_N) = c^2 \\sum_{n=1}^N x_n\n",
    "$$\n",
    "\n",
    "と「くくり出す」ことができます。行列やベクトルの場合も、これと全く同じ原理に基づいています。\n",
    "\n",
    "### 理由2：行列の乗算における「分配法則」\n",
    "\n",
    "理由1の「定数だから外に出せる」という直感を、数学的に保証しているのが**行列の分配法則**です。\n",
    "\n",
    "スカラーで $A(B+C) = AB+AC$ が成り立つように、行列でも（次元が合う限り）同様の法則が成り立ちます。\n",
    "\n",
    "では、実際に総和の式を書き下して、分配法則を適用してみましょう。\n",
    "\n",
    "$$\\sum_{n=1}^N \\boldsymbol{a}^T\\boldsymbol{x}_n\\boldsymbol{x}_n^T\\boldsymbol{a} = (\\boldsymbol{a}^T\\boldsymbol{x}_1\\boldsymbol{x}_1^T\\boldsymbol{a}) + (\\boldsymbol{a}^T\\boldsymbol{x}_2\\boldsymbol{x}_2^T\\boldsymbol{a}) + \\dots + (\\boldsymbol{a}^T\\boldsymbol{x}_N\\boldsymbol{x}_N^T\\boldsymbol{a})$$\n",
    "\n",
    "ここで、各項は `(ベクトル) * (行列) * (ベクトル)` の形をしています。\n",
    "\n",
    "**ステップA：左側から $\\boldsymbol{a}^T$ をくくり出す**\n",
    "行列の分配法則 `AB + AC = A(B+C)` を使います。\n",
    "`A` を $\\boldsymbol{a}^T$、`B` を $\\boldsymbol{x}_1\\boldsymbol{x}_1^T\\boldsymbol{a}$、`C` を $\\boldsymbol{x}_2\\boldsymbol{x}_2^T\\boldsymbol{a}$... と見なすと、\n",
    "\n",
    "$$= \\boldsymbol{a}^T \\left[ (\\boldsymbol{x}_1\\boldsymbol{x}_1^T\\boldsymbol{a}) + (\\boldsymbol{x}_2\\boldsymbol{x}_2^T\\boldsymbol{a}) + \\dots + (\\boldsymbol{x}_N\\boldsymbol{x}_N^T\\boldsymbol{a}) \\right]$$\n",
    "\n",
    "と、左側の $\\boldsymbol{a}^T$ を共通因子として括弧の外に出せます。\n",
    "\n",
    "**ステップB：右側から $\\boldsymbol{a}$ をくくり出す**\n",
    "次に、行列の分配法則 `BA + CA = (B+C)A` を使います。\n",
    "角括弧 `[]` の中で、`A` を $\\boldsymbol{a}$、`B` を $\\boldsymbol{x}_1\\boldsymbol{x}_1^T$、`C` を $\\boldsymbol{x}_2\\boldsymbol{x}_2^T$... と見なすと、\n",
    "\n",
    "$$= \\boldsymbol{a}^T \\left[ (\\boldsymbol{x}_1\\boldsymbol{x}_1^T + \\boldsymbol{x}_2\\boldsymbol{x}_2^T + \\dots + \\boldsymbol{x}_N\\boldsymbol{x}_N^T) \\boldsymbol{a} \\right]$$\n",
    "\n",
    "と、右側の $\\boldsymbol{a}$ を共通因子として括弧の外に出せます。\n",
    "\n",
    "**ステップC：総和記号に戻す**\n",
    "最後に、角括弧の中の行列の和を、総和記号 $\\sum$ を使って簡潔に表現します。\n",
    "\n",
    "$$= \\boldsymbol{a}^T \\left( \\sum_{n=1}^N \\boldsymbol{x}_n\\boldsymbol{x}_n^T \\right) \\boldsymbol{a}$$\n",
    "\n",
    "これで、目的の変形が完了しました。\n",
    "\n",
    "### まとめ\n",
    "\n",
    "くくり出しが可能である根拠は、以下の2つの数学的事実に基づいています。\n",
    "\n",
    "1.  **定数性**: くくり出したいベクトル $\\boldsymbol{a}$ は、総和を計算する際のインデックス $n$ に依存しないため、**定数**として扱える。\n",
    "2.  **分配法則**: 行列の乗算には、スカラー計算と同様に**分配法則**が成り立つため、共通の因子を式の外に出すことができる。\n",
    "\n",
    "この操作は、一見すると複雑な「N個の項の和」を、「一つの行列に対する二次形式」という、非常にシンプルで扱いやすい形に変換するための、強力な数学的テクニックなのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、承知いたしました。ご依頼の通り、行列の積 $Y=XW$ を成分単位で計算し、その結果、$Y$ の各成分 $Y_{ij}$ が、i番目のデータ点 $x_i$ をj番目の主成分 $w_j$ へ射影した点（座標値）であることを証明します。\n",
    "\n",
    "---\n",
    "\n",
    "### 証明\n",
    "\n",
    "#### 1. 前提：各行列の定義\n",
    "\n",
    "まず、計算に使用する行列 $X, W, Y$ を成分で定義します。\n",
    "\n",
    "* **データ行列 $X$ (サイズ: $n \\times p$)**\n",
    "    $i$ 番目のデータ点（$i$行目）をベクトル $x_i = (x_{i1}, x_{i2}, \\dots, x_{ip})$ とします。\n",
    "    $$\n",
    "    X =\n",
    "    \\begin{pmatrix}\n",
    "    x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{i1} & x_{i2} & \\dots & x_{ip} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "* **主成分負荷量行列 $W$ (サイズ: $p \\times q$)**\n",
    "    $j$ 番目の主成分ベクトル（$j$列目）を $w_j = (w_{1j}, w_{2j}, \\dots, w_{pj})^T$ とします。\n",
    "    ※ここでの添え字に注意してください。$w_{kj}$ は、$j$番目の主成分ベクトルの$k$番目の成分を意味します。\n",
    "    $$\n",
    "    W =\n",
    "    \\begin{pmatrix}\n",
    "    w_{11} & \\dots & w_{1j} & \\dots & w_{1q} \\\\\n",
    "    w_{21} & \\dots & w_{2j} & \\dots & w_{2q} \\\\\n",
    "    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{p1} & \\dots & w_{pj} & \\dots & w_{pq}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "* **射影後のデータ行列 $Y$ (サイズ: $n \\times q$)**\n",
    "    この行列の $i$ 行 $j$ 列の成分を $Y_{ij}$ とします。私たちの目標は、$Y_{ij}$ の正体を明らかにすることです。\n",
    "    $$\n",
    "    Y =\n",
    "    \\begin{pmatrix}\n",
    "    Y_{11} & \\dots & Y_{1j} & \\dots & Y_{1q} \\\\\n",
    "    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    Y_{i1} & \\dots & Y_{ij} & \\dots & Y_{iq} \\\\\n",
    "    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    Y_{n1} & \\dots & Y_{nj} & \\dots & Y_{nq}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "#### 2. 証明の目標\n",
    "\n",
    "行列の積 $Y = XW$ の成分 $Y_{ij}$ が、データ点 $x_i$ と主成分ベクトル $w_j$ の**内積**と等しくなることを示す。\n",
    "すなわち、以下を証明する。\n",
    "\n",
    "$$Y_{ij} = x_i \\cdot w_j$$\n",
    "\n",
    "なぜなら、あるベクトル ($x_i$) と**単位ベクトル** ($w_j$ は単位ベクトル) の内積は、そのベクトルを単位ベクトルの張る直線へ正射影したときの座標値（符号付き長さ）の定義そのものであるからです。\n",
    "\n",
    "#### 3. 成分による計算の実行\n",
    "\n",
    "行列の積の定義によれば、$Y$ の $i$ 行 $j$ 列成分 $Y_{ij}$ は、$X$ の $i$ 行目と $W$ の $j$ 列目の要素をそれぞれ掛け合わせて総和をとったものになります。\n",
    "\n",
    "数式で書くと、以下のようになります。\n",
    "\n",
    "$$Y_{ij} = \\sum_{k=1}^{p} X_{ik} W_{kj}$$\n",
    "\n",
    "ここで、$X$ と $W$ の成分の定義を代入します。\n",
    "\n",
    "* $X$ の $i$ 行 $k$ 列の成分 $X_{ik}$ は、$x_{ik}$ です。\n",
    "* $W$ の $k$ 行 $j$ 列の成分 $W_{kj}$ は、$w_{kj}$ です。\n",
    "\n",
    "したがって、\n",
    "\n",
    "$$Y_{ij} = \\sum_{k=1}^{p} x_{ik} w_{kj}$$\n",
    "\n",
    "この総和($\\sum$)を具体的に書き下してみましょう。\n",
    "\n",
    "$$Y_{ij} = x_{i1}w_{1j} + x_{i2}w_{2j} + x_{i3}w_{3j} + \\dots + x_{ip}w_{pj}$$\n",
    "\n",
    "この式は、2つのベクトルの内積の定義と完全に一致します。\n",
    "その2つのベクトルとは、\n",
    "\n",
    "* **$i$ 番目のデータ点ベクトル:** $x_i = (x_{i1}, x_{i2}, \\dots, x_{ip})$\n",
    "* **$j$ 番目の主成分ベクトル:** $w_j = (w_{1j}, w_{2j}, \\dots, w_{pj})$\n",
    "\n",
    "です。\n",
    "\n",
    "よって、\n",
    "\n",
    "$$Y_{ij} = x_i \\cdot w_j$$\n",
    "\n",
    "が成り立ちます。\n",
    "\n",
    "#### 4. 結論\n",
    "\n",
    "行列の積 $Y = XW$ を成分で計算した結果、$Y$ の $i$ 行 $j$ 列の成分 $Y_{ij}$ は、$i$ 番目のデータ点ベクトル $x_i$ と $j$ 番目の主成分ベクトル $w_j$ の内積に等しいことが明らかになりました。\n",
    "\n",
    "主成分ベクトルは単位ベクトルであるため、この内積は、ベクトル $x_i$ を、ベクトル $w_j$ が定める直線（主成分軸）へ正射影したときの、原点からの符号付きの長さ（座標）を意味します。\n",
    "\n",
    "したがって、行列 $Y$ の各成分は、各データ点を各主成分軸へ射影した点（の座標値）であることが証明されました。\n",
    "\n",
    "**[証明終]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、承知いたしました。\n",
    "これまでの一般的な議論を、具体的な2次元データに当てはめて、計算の流れを一つ一つ確認していきましょう。\n",
    "\n",
    "「適性検査1 ($x_1$)」と「適性検査2 ($x_2$)」のスコアからなる、3人分の簡単なデータセットを例に解説します。\n",
    "\n",
    "### Step 0: 具体的なデータセットの準備\n",
    "\n",
    "| 受験者 | 適性検査1 ($x_1$) | 適性検査2 ($x_2$) |\n",
    "| :--- | :--- | :--- |\n",
    "| Aさん | 1 | 2 |\n",
    "| Bさん | 3 | 3 |\n",
    "| Cさん | 5 | 7 |\n",
    "\n",
    "この3つのデータ点 (1, 2), (3, 3), (5, 7) が、私たちの出発点です。\n",
    "\n",
    "### Step 1: データ行列 $X$ の作成と中心化\n",
    "\n",
    "まず、このデータを $n \\times p$ （ここでは $3 \\times 2$）の行列 $X$ として表現します。\n",
    "\n",
    "$$X = \\begin{pmatrix} 1 & 2 \\\\ 3 & 3 \\\\ 5 & 7 \\end{pmatrix}$$\n",
    "\n",
    "次に、各列（変数）の平均値を計算し、各要素から引くことで**中心化**を行います。\n",
    "\n",
    "* $x_1$ の平均: $\\mu_1 = (1+3+5)/3 = 3$\n",
    "* $x_2$ の平均: $\\mu_2 = (2+3+7)/3 = 4$\n",
    "\n",
    "中心化されたデータ行列 $X_c$ は以下のようになります。\n",
    "\n",
    "$$\n",
    "X_c =\n",
    "\\begin{pmatrix}\n",
    "1-3 & 2-4 \\\\\n",
    "3-3 & 3-4 \\\\\n",
    "5-3 & 7-4\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "-2 & -2 \\\\\n",
    "0 & -1 \\\\\n",
    "2 & 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Step 2: 共分散行列 $S$ の計算\n",
    "\n",
    "次に、データ全体のばらつきと変数間の関係性を要約した**共分散行列 $S$** を計算します。\n",
    "（※不偏分散を求めるため、データ数 $n-1=2$ で割ります）\n",
    "\n",
    "$$S = \\frac{1}{n-1} X_c^T X_c = \\frac{1}{2} \\begin{pmatrix} -2 & 0 & 2 \\\\ -2 & -1 & 3 \\end{pmatrix} \\begin{pmatrix} -2 & -2 \\\\ 0 & -1 \\\\ 2 & 3 \\end{pmatrix}$$\n",
    "\n",
    "$$S = \\frac{1}{2} \\begin{pmatrix} (-2)(-2)+0+2(2) & (-2)(-2)+0+(-1)+2(3) \\\\ (-2)(-2)+(-1)0+3(2) & (-2)(-2)+(-1)(-1)+3(3) \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 8 & 10 \\\\ 10 & 14 \\end{pmatrix} = \\begin{pmatrix} 4 & 5 \\\\ 5 & 7 \\end{pmatrix}$$\n",
    "\n",
    "これが私たちの分析の土台となる共分散行列 $S$ です。\n",
    "\n",
    "### Step 3: 固有値 ($\\lambda$) と固有ベクトル ($w$) の計算\n",
    "\n",
    "$S$ の固有値と固有ベクトルを求めます。特性方程式 $\\det(S - \\lambda I) = 0$ を解きます。\n",
    "\n",
    "$$\\det \\left( \\begin{pmatrix} 4-\\lambda & 5 \\\\ 5 & 7-\\lambda \\end{pmatrix} \\right) = (4-\\lambda)(7-\\lambda) - 25 = 0$$\n",
    "$$\\lambda^2 - 11\\lambda + 28 - 25 = 0$$\n",
    "$$\\lambda^2 - 11\\lambda + 3 = 0$$\n",
    "\n",
    "解の公式より、$\\lambda = \\frac{11 \\pm \\sqrt{121-12}}{2} = \\frac{11 \\pm \\sqrt{109}}{2}$。\n",
    "\n",
    "* **第1主成分の固有値 $\\lambda_1$**: $\\lambda_1 = \\frac{11 + \\sqrt{109}}{2} \\approx 10.72$\n",
    "* **第2主成分の固有値 $\\lambda_2$**: $\\lambda_2 = \\frac{11 - \\sqrt{109}}{2} \\approx 0.28$\n",
    "\n",
    "今回は次元削減が目的なので、より大きな固有値を持つ**第1主成分** $\\lambda_1 \\approx 10.72$ に対応する固有ベクトル $w_1$ を求めます。\n",
    "$(S - \\lambda_1 I)w = 0$ を解くと、固有ベクトル $w_1$ はおよそ以下のようになります（計算略）。\n",
    "\n",
    "$$w_1 \\propto \\begin{pmatrix} 0.61 \\\\ 0.79 \\end{pmatrix}$$\n",
    "\n",
    "これはデータのばらつきが最も大きい方向、つまり新しい主軸の方向です。\n",
    "\n",
    "### Step 4: 射影行列 $W$ の作成\n",
    "\n",
    "2次元から1次元へ削減したいので、$q=1$ です。したがって、射影行列 $W$ は、先ほど求めた $w_1$ を列ベクトルとして持つ、$2 \\times 1$ の行列になります。\n",
    "\n",
    "$$W = \\begin{pmatrix} 0.61 \\\\ 0.79 \\end{pmatrix}$$\n",
    "\n",
    "### Step 5: 射影の実行 $Y = X_c W$\n",
    "\n",
    "いよいよ、中心化したデータ $X_c$ を新しい軸 $W$ に射影します。\n",
    "\n",
    "$$Y = X_c W = \\begin{pmatrix} -2 & -2 \\\\ 0 & -1 \\\\ 2 & 3 \\end{pmatrix} \\begin{pmatrix} 0.61 \\\\ 0.79 \\end{pmatrix}$$\n",
    "\n",
    "$$Y = \\begin{pmatrix} (-2)(0.61) + (-2)(0.79) \\\\ (0)(0.61) + (-1)(0.79) \\\\ (2)(0.61) + (3)(0.79) \\end{pmatrix} = \\begin{pmatrix} -2.8 \\\\ -0.79 \\\\ 3.59 \\end{pmatrix}$$\n",
    "\n",
    "この $Y$ が最終結果です。Aさん、Bさん、Cさんのデータは、新しい1次元の軸の上で、それぞれ -2.8, -0.79, 3.59 という座標値に変換されました。元の2つのテストスコアが、**「総合的な適性」**という1つの指標にまとめられたと解釈できます。\n",
    "\n",
    "### Step 6: 一般論との接続と証明の確認\n",
    "\n",
    "最後に、先ほどの証明を**具体的なデータで確認**してみましょう。\n",
    "例えば、$Y$ の最初の成分 $Y_{11} = -2.8$ は、Aさんのデータから計算されました。\n",
    "\n",
    "**証明の確認**: $Y_{11}$ は、Aさんの中心化データ $x_{c,A}$ と主成分ベクトル $w_1$ の内積と等しいか？\n",
    "\n",
    "* **Aさんの中心化データ**: $x_{c,A} = (-2, -2)$\n",
    "* **第1主成分ベクトル**: $w_1 = (0.61, 0.79)$\n",
    "\n",
    "**内積を計算**:\n",
    "$$x_{c,A} \\cdot w_1 = (-2)(0.61) + (-2)(0.79) = -1.22 - 1.58 = -2.8$$\n",
    "\n",
    "これは、行列計算 $Y=X_cW$ の結果と**完全に一致**します。\n",
    "\n",
    "このように、一般的な理論であった $Y=XW$ という抽象的な式が、具体的なデータに適用すると、一つ一つのデータ点（受験者）を新しい主成分軸（総合的な適性の軸）に「影を落とす（射影する）」という直感的な操作に対応していることが確認できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データがp次元なら、主成分の単位ベクトルとうか基底ベクトル？はp次元なんじゃないすか。んで、Ｗはp次元のデータn行の行列(n行p列)を迎合するように、p行q列で設計し、それぞれのデータ(行ベクトル)に対して、各固有ベクトル(Ｗの各列)をp - (削減したい分の次数)回数分、作用させ、ここでは例えばq次元に削減されているが、削減後のデータはn行p列の行列となっている。このことを、数式で分かりやすく説明しながら補足して。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、ご指摘の点は核心を突いた非常に良い質問です。その思考プロセスを数式で補足しながら、少しだけ認識を修正することで、より正確な理解に繋がります。\n",
    "\n",
    "前半の理解は全くその通りです。\n",
    "\n",
    "> データがp次元なら、主成分の単位ベクトルとうか基底ベクトル？はp次元なんじゃないすか。\n",
    "\n",
    "**その通りです。** 元のデータが存在するp次元空間内の「方向」を指し示すため、主成分ベクトル（新しい基底ベクトル）$w_j$ は必ずp次元のベクトルになります。\n",
    "\n",
    "> んで、Ｗはp次元のデータn行の行列(n行p列)を迎合するように、p行q列で設計し…\n",
    "\n",
    "**その通りです。** データ行列 $X$ ($n \\times p$) との行列積が成立するように、また、新しい次元がq次元になるように、射影行列 $W$ は ($p \\times q$) となります。\n",
    "\n",
    "ここからが、今回の質問の核心部分です。\n",
    "\n",
    "> ...それぞれのデータ(行ベクトル)に対して、各固有ベクトル(Ｗの各列)をp - (削減したい分の次数)回数分、作用させ、ここでは例えばq次元に削減されているが、削減後のデータはn行p列の行列となっている。\n",
    "\n",
    "この部分の**「作用させる回数」**と**「削減後のデータの形」**が、実際の計算とは少し異なります。正しくは、各データ点に対して、選ばれた**q個の固有ベクトルをそれぞれ1回ずつ作用させ、q個の新しい座標値を得る**、となります。\n",
    "\n",
    "このことを数式で解説します。\n",
    "\n",
    "---\n",
    "\n",
    "### 数式による補足説明\n",
    "\n",
    "#### 1. 登場人物の確認\n",
    "\n",
    "* **$X$ (データ行列)**: サイズ $n \\times p$。$i$行目はデータ点 $x_i = (x_{i1}, \\dots, x_{ip})$ です。\n",
    "* **$W$ (射影行列)**: サイズ $p \\times q$。$j$列目は主成分ベクトル $w_j = (w_{1j}, \\dots, w_{pj})^T$ です。\n",
    "* **$Y$ (削減後のデータ行列)**: 私たちが求めたい行列です。\n",
    "\n",
    "#### 2. 「作用させる」とは何か？ (1つのデータ点に注目)\n",
    "\n",
    "まず、ある**1つのデータ点**（$i$行目のベクトル $x_i$）がどう変換されるかを見てみましょう。\n",
    "$x_i$ は $1 \\times p$ の行ベクトルです。これに行列 $W$ を右から掛け合わせます。\n",
    "\n",
    "$$y_i = x_i W$$\n",
    "\n",
    "この計算は、具体的に何をしているのでしょうか。$W$ を列ベクトルの集まりとして見てみましょう。\n",
    "$W = [w_1 | w_2 | \\dots | w_q]$\n",
    "\n",
    "$$y_i = x_i [w_1 | w_2 | \\dots | w_q]$$\n",
    "\n",
    "行列の積の性質から、この結果 $y_i$ は $1 \\times q$ の新しい行ベクトルになります。そして、その各成分は以下のようになります。\n",
    "\n",
    "$$y_i = ( \\underbrace{x_i \\cdot w_1}_{\\text{第1成分}}, \\underbrace{x_i \\cdot w_2}_{\\text{第2成分}}, \\dots, \\underbrace{x_i \\cdot w_q}_{\\text{第q成分}} )$$\n",
    "\n",
    "**ここが最も重要なポイントです。**\n",
    "この一行の計算だけで、\n",
    "\n",
    "* 元のデータ点 $x_i$ を、**第1主成分 $w_1$ に射影した値**（内積）が、新しい座標の**第1成分**になり、\n",
    "* 元のデータ点 $x_i$ を、**第2主成分 $w_2$ に射影した値**が、新しい座標の**第2成分**になり、\n",
    "* ...\n",
    "* 元のデータ点 $x_i$ を、**第q主成分 $w_q$ に射影した値**が、新しい座標の**第q成分**になります。\n",
    "\n",
    "つまり、「作用させる」という操作は、削減したい次元数（$p-q$）だけ何かを繰り返すのではなく、**削減後の次元数q個の新しい軸（基底ベクトル）それぞれに1回ずつ射影して、q個の新しい座標値を一度に得ること**を意味します。\n",
    "\n",
    "#### 3. 全データへの拡張と削減後のデータの形\n",
    "\n",
    "上記の操作を、1番目のデータ点からn番目のデータ点まで、すべての行に対して一括で行うのが、行列積 $Y=XW$ です。\n",
    "\n",
    "$$\n",
    "Y = XW =\n",
    "\\begin{pmatrix}\n",
    "\\leftarrow & x_1 & \\rightarrow \\\\\n",
    "\\leftarrow & x_2 & \\rightarrow \\\\\n",
    "& \\vdots & \\\\\n",
    "\\leftarrow & x_n & \\rightarrow\n",
    "\\end{pmatrix}\n",
    "[w_1 | w_2 | \\dots | w_q]\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\leftarrow & x_1W & \\rightarrow \\\\\n",
    "\\leftarrow & x_2W & \\rightarrow \\\\\n",
    "& \\vdots & \\\\\n",
    "\\leftarrow & x_nW & \\rightarrow\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\cdot w_1 & x_1 \\cdot w_2 & \\dots & x_1 \\cdot w_q \\\\\n",
    "x_2 \\cdot w_1 & x_2 \\cdot w_2 & \\dots & x_2 \\cdot w_q \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_n \\cdot w_1 & x_n \\cdot w_2 & \\dots & x_n \\cdot w_q\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "この最終的な行列 $Y$ のサイズを見てみましょう。\n",
    "行数はデータ数と同じ $n$、列数は新しい次元数である $q$ です。\n",
    "\n",
    "したがって、**削減後のデータは $n \\times q$ の行列**となります。\n",
    "\n",
    "### まとめ\n",
    "\n",
    "| | 開始 (Before) | 変換 (Transformation) | 結果 (After) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **データの形** | **$X$ (n行p列)** | $W$ (p行q列) を右から掛ける | **$Y$ (n行q列)** |\n",
    "| **1つのデータ** | $p$次元の点 | $q$個の新しい軸へ一度に射影 | $q$次元の点 |\n",
    "| **次元数** | $p$ | | **$q$** |\n",
    "\n",
    "このように、$Y=XW$ というシンプルな式は、「n個のp次元データを、q個の新しい基底ベクトルが張る空間に射影し、n個のq次元データに変換する」という操作を、極めてエレガントに表現しています。削減後のデータは、元の次元数pの情報は持たず、新しいq次元の情報のみを持つことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、その認識で間違いありません。非常に鋭いポイントです。\n",
    "\n",
    "主成分負荷量を計算する際に、**共分散行列を使うか、相関行列を使うかで、得られる負荷量（固有ベクトル）も、その解釈に使う公式も異なります。**\n",
    "\n",
    "これは、分析の出発点となる行列が違うため、結果として得られる「主成分」の意味合いが変わってくるからです。\n",
    "\n",
    "この違いを、簡単な具体例を交えて解説します。\n",
    "\n",
    "---\n",
    "\n",
    "### 簡単な具体問題\n",
    "\n",
    "**シナリオ:**\n",
    "2人の生徒の「小テスト（10点満点）」と「期末試験（100点満点）」の2つの成績データがあります。変数のスケール（満点）が大きく異なる点に注目してください。\n",
    "\n",
    "| 生徒 | 小テスト ($x_1$) | 期末試験 ($x_2$) |\n",
    "| :--- | :--- | :--- |\n",
    "| Aさん | 1 | 10 |\n",
    "| Bさん | 3 | 50 |\n",
    "\n",
    "### ケース1：共分散行列を用いた場合\n",
    "\n",
    "#### 1. 共分散行列 $S$ の計算\n",
    "* 平均: $\\mu_1 = 2$, $\\mu_2 = 30$\n",
    "* 中心化データ $X_c = \\begin{pmatrix} -1 & -20 \\\\ 1 & 20 \\end{pmatrix}$\n",
    "* 共分散行列 $S = \\frac{1}{2-1}X_c^T X_c = \\begin{pmatrix} 2 & 40 \\\\ 40 & 800 \\end{pmatrix}$\n",
    "\n",
    "#### 2. 固有値・固有ベクトルの計算\n",
    "この $S$ の固有値と固有ベクトル（=主成分負荷量 Definition A）を求めると、以下のようになります。\n",
    "* $\\lambda_{S1} \\approx 800.02$, $\\quad w_1 \\approx \\begin{pmatrix} 0.05 \\\\ 0.999 \\end{pmatrix}$\n",
    "* $\\lambda_{S2} \\approx 1.98$, $\\quad w_2 \\approx \\begin{pmatrix} -0.999 \\\\ 0.05 \\end{pmatrix}$\n",
    "\n",
    "第1主成分の負荷量ベクトル $w_1$ を見ると、期末試験($x_2$)の重みが0.999と圧倒的に大きく、小テスト($x_1$)の重みは0.05と非常に小さいです。これは、**分散が巨大な期末試験の成績に、主成分が強く引っ張られた**ことを意味します。\n",
    "\n",
    "#### 3. 負荷量の解釈（相関を求める）\n",
    "では、この主成分スコア $y_1 = X_c w_1$ と、元の変数 $x_1, x_2$ との相関を計算してみます。\n",
    "相関の公式は、ご指摘の通り、共分散行列の場合は以下のようになります。\n",
    "$$\\text{Corr}(x_k, y_j) = \\frac{w_{kj} \\sqrt{\\lambda_{Sj}}}{\\sigma_k}$$\n",
    "* $\\sigma_1 = \\sqrt{S_{11}} = \\sqrt{2} \\approx 1.41$\n",
    "* $\\sigma_2 = \\sqrt{S_{22}} = \\sqrt{800} \\approx 28.28$\n",
    "\n",
    "* **小テストとの相関**: $\\text{Corr}(x_1, y_1) = \\frac{0.05 \\times \\sqrt{800.02}}{1.41} \\approx 1.0$\n",
    "* **期末試験との相関**: $\\text{Corr}(x_2, y_1) = \\frac{0.999 \\times \\sqrt{800.02}}{28.28} \\approx 1.0$\n",
    "\n",
    "この場合、両方とも相関が1.0となり、解釈が少し難しくなります。重要なのは、**固有ベクトル $w_1$ の要素の大きさと、解釈のための相関値の大きさが、単純に比例しない**点です。\n",
    "\n",
    "---\n",
    "\n",
    "### ケース2：相関行列を用いた場合\n",
    "\n",
    "#### 1. 相関行列 $R$ の計算\n",
    "まずデータを標準化（平均0, 標準偏差1に変換）します。\n",
    "* 標準化データ $Z = \\begin{pmatrix} -0.707 & -0.707 \\\\ 0.707 & 0.707 \\end{pmatrix}$\n",
    "* 相関行列 $R = \\frac{1}{2-1}Z^T Z = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$\n",
    "\n",
    "#### 2. 固有値・固有ベクトルの計算\n",
    "この $R$ の固有値と固有ベクトル（=主成分負荷量 Definition A）を求めると、以下のようになります。\n",
    "* $\\lambda_{R1} = 2$, $\\quad v_1 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix} \\approx \\begin{pmatrix} 0.707 \\\\ 0.707 \\end{pmatrix}$\n",
    "* $\\lambda_{R2} = 0$, $\\quad v_2 = \\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix} \\approx \\begin{pmatrix} -0.707 \\\\ 0.707 \\end{pmatrix}$\n",
    "\n",
    "第1主成分の負荷量ベクトル $v_1$ を見ると、小テスト($x_1$)と期末試験($x_2$)の重みが共に0.707で**等しくなっています**。これは、**変数のスケールを標準化で揃えたため、両方の変数が平等に考慮された**結果です。\n",
    "\n",
    "#### 3. 負荷量の解釈（相関を求める）\n",
    "相関行列を用いた場合、相関を求める公式はシンプルになります。\n",
    "$$\\text{Corr}(x_k, y_j) = v_{kj} \\sqrt{\\lambda_{Rj}}$$\n",
    "* **小テストとの相関**: $\\text{Corr}(x_1, y_1) = 0.707 \\times \\sqrt{2} \\approx 1.0$\n",
    "* **期末試験との相関**: $\\text{Corr}(x_2, y_1) = 0.707 \\times \\sqrt{2} \\approx 1.0$\n",
    "\n",
    "この場合、**固有ベクトル $v_1$ の要素の大きさと、解釈のための相関値の大きさが、単純に比例**します。$v_1$ の要素が等しいので、相関値も等しくなります。\n",
    "\n",
    "---\n",
    "\n",
    "### まとめ：違いは何か？\n",
    "\n",
    "| 比較項目 | 共分散行列 $S$ を使用 | 相関行列 $R$ を使用 |\n",
    "| :--- | :--- | :--- |\n",
    "| **分析対象** | 元のデータそのもの（スケール維持） | 標準化されたデータ（スケール統一） |\n",
    "| **固有ベクトル($w_j, v_j$)** | **分散の大きい変数**に強く影響される。 | 全ての変数を**平等に**扱う。 |\n",
    "| **負荷量の解釈公式** | $\\text{Corr}(x_k, y_j) = \\frac{w_{kj} \\sqrt{\\lambda_{Sj}}}{\\sigma_k}$ | $\\text{Corr}(x_k, y_j) = v_{kj} \\sqrt{\\lambda_{Rj}}$ |\n",
    "| **どんな時に使うか** | 全変数の単位やスケールが**同じ**で、分散の大きさを「重要度」とみなしたい時。 | 変数の単位やスケールが**異なる**時。（**一般的にはこちらを使うことが多い**） |\n",
    "\n",
    "**結論として、ご認識の通り「公式が異なる」は真実です。**\n",
    "\n",
    "より正確に言えば、固有ベクトルを求めるという数学的なプロセスは同じですが、**出発点となる行列が違うため、得られる固有ベクトル（負荷量）の意味合いが全く変わってきます**。そして、その負荷量を解釈（元変数との相関を計算）する際の換算式も、上記のように異なってくるのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、承知いたしました。\n",
    "「主成分負荷量」を、元変数と主成分スコアとの間の関連性として定義した場合、その計算式がなぜあの形になるのか。共分散行列を用いた場合と、相関行列を用いた場合、それぞれのケースについて数学的な証明を行います。\n",
    "\n",
    "---\n",
    "\n",
    "### 証明1：共分散行列を用いた場合\n",
    "\n",
    "この場合の目標は、以下の公式を証明することです。\n",
    "\n",
    "$$\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\lambda_j w_{kj}$$\n",
    "\n",
    "**【前提】**\n",
    "* $X_c$: $n \\times p$ の中心化データ行列。\n",
    "* $\\boldsymbol{x}_k$: $X_c$ の $k$番目の列ベクトル（$k$番目の変数の全データ）。\n",
    "* $S$: 共分散行列。$S = \\frac{1}{n-1}X_c^T X_c$。\n",
    "* $\\lambda_j, \\boldsymbol{w}_j$: $S$ の $j$番目の固有値と固有ベクトル。$S\\boldsymbol{w}_j = \\lambda_j \\boldsymbol{w}_j$ が成立。\n",
    "* $\\boldsymbol{y}_j$: 第$j$主成分スコアベクトル。$\\boldsymbol{y}_j = X_c \\boldsymbol{w}_j$ で計算される。\n",
    "\n",
    "**【証明】**\n",
    "\n",
    "1.  まず、$k$番目の変数 $\\boldsymbol{x}_k$ と $j$番目のスコア $\\boldsymbol{y}_j$ の共分散の定義から出発します。データは中心化されているため、共分散はベクトルの内積を $n-1$ で割ることで計算できます。\n",
    "    $$\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\frac{1}{n-1} \\boldsymbol{x}_k^T \\boldsymbol{y}_j$$\n",
    "\n",
    "2.  ここに、スコアの定義式 $\\boldsymbol{y}_j = X_c \\boldsymbol{w}_j$ を代入します。\n",
    "    $$\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\frac{1}{n-1} \\boldsymbol{x}_k^T (X_c \\boldsymbol{w}_j)$$\n",
    "\n",
    "3.  ここで、ベクトル $\\boldsymbol{x}_k$ は行列 $X_c$ の $k$番目の列である、という点に注目します。この関係を利用するため、一度すべての変数ベクトルとスコアベクトル $\\boldsymbol{y}_j$ の共分散をまとめたベクトルを考えます。これは $p \\times 1$ のベクトルになります。\n",
    "    $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = \\frac{1}{n-1} X_c^T \\boldsymbol{y}_j$$\n",
    "\n",
    "4.  この式に、再び $\\boldsymbol{y}_j = X_c \\boldsymbol{w}_j$ を代入します。\n",
    "    $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = \\frac{1}{n-1} X_c^T (X_c \\boldsymbol{w}_j) = \\left(\\frac{1}{n-1} X_c^T X_c\\right) \\boldsymbol{w}_j$$\n",
    "\n",
    "5.  括弧の中の $\\frac{1}{n-1} X_c^T X_c$ は、共分散行列 $S$ の定義そのものです。\n",
    "    $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = S \\boldsymbol{w}_j$$\n",
    "\n",
    "6.  ここで、固有値・固有ベクトルの定義式 $S \\boldsymbol{w}_j = \\lambda_j \\boldsymbol{w}_j$ を用いると、\n",
    "    $$\\text{Cov}(X_c, \\boldsymbol{y}_j) = \\lambda_j \\boldsymbol{w}_j$$\n",
    "\n",
    "7.  この等式は、$p \\times 1$ のベクトル同士が等しいことを示しています。左辺のベクトルの第$k$要素は $\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j)$ であり、右辺のベクトルの第$k$要素は $\\lambda_j w_{kj}$ です。両ベクトルの対応する要素は等しいので、\n",
    "    $$\\text{Cov}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\lambda_j w_{kj}$$\n",
    "\n",
    "**[証明終]**\n",
    "\n",
    "---\n",
    "\n",
    "### 証明2：相関行列を用いた場合\n",
    "\n",
    "この場合の目標は、以下の公式を証明することです。\n",
    "\n",
    "$$\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = v_{kj} \\sqrt{\\lambda'_j}$$\n",
    "\n",
    "**【前提】**\n",
    "* $Z$: $n \\times p$ の標準化データ行列（平均0, 標準偏差1）。\n",
    "* $R$: 相関行列。$R = \\frac{1}{n-1}Z^T Z$。\n",
    "* $\\lambda'_j, \\boldsymbol{v}_j$: $R$ の $j$番目の固有値と固有ベクトル。$R\\boldsymbol{v}_j = \\lambda'_j \\boldsymbol{v}_j$ が成立。\n",
    "* $\\boldsymbol{y}_j$: 第$j$主成分スコアベクトル。$\\boldsymbol{y}_j = Z \\boldsymbol{v}_j$ で計算される。\n",
    "* $\\boldsymbol{z}_k$: $Z$ の $k$番目の列ベクトル（$k$番目の標準化済み変数）。\n",
    "\n",
    "**【証明】**\n",
    "\n",
    "1.  相関係数の定義式から出発します。\n",
    "    $$\\text{Corr}(\\boldsymbol{z}_k, \\boldsymbol{y}_j) = \\frac{\\text{Cov}(\\boldsymbol{z}_k, \\boldsymbol{y}_j)}{\\sigma_{z_k} \\sigma_{y_j}}$$\n",
    "\n",
    "2.  分子の $\\text{Cov}(\\boldsymbol{z}_k, \\boldsymbol{y}_j)$ を考えます。これは**証明1と全く同じ構造**です。分析対象を行列 $Z$、共分散行列を $R$、固有値を $\\lambda'_j$、固有ベクトルを $\\boldsymbol{v}_j$ と置き換えれば、証明1の結果をそのまま適用できます。\n",
    "    $$\\text{Cov}(\\boldsymbol{z}_k, \\boldsymbol{y}_j) = \\lambda'_j v_{kj}$$\n",
    "\n",
    "3.  分母の標準偏差を考えます。\n",
    "    * $\\sigma_{z_k}$: $\\boldsymbol{z}_k$ は標準化された変数なので、その標準偏差は定義により $1$ です。$(\\sigma_{z_k} = 1)$\n",
    "    * $\\sigma_{y_j}$: スコア $\\boldsymbol{y}_j$ の分散を計算します。\n",
    "    $\\text{Var}(\\boldsymbol{y}_j) = \\text{Var}(Z\\boldsymbol{v}_j) = \\boldsymbol{v}_j^T \\text{Cov}(Z) \\boldsymbol{v}_j = \\boldsymbol{v}_j^T R \\boldsymbol{v}_j$\n",
    "    ここで $R\\boldsymbol{v}_j = \\lambda'_j \\boldsymbol{v}_j$ なので、\n",
    "    $\\text{Var}(\\boldsymbol{y}_j) = \\boldsymbol{v}_j^T (\\lambda'_j \\boldsymbol{v}_j) = \\lambda'_j (\\boldsymbol{v}_j^T \\boldsymbol{v}_j)$\n",
    "    $\\boldsymbol{v}_j$ は正規化されているので $\\boldsymbol{v}_j^T \\boldsymbol{v}_j = 1$ です。したがって、$\\text{Var}(\\boldsymbol{y}_j) = \\lambda'_j$。\n",
    "    よって、標準偏差は $\\sigma_{y_j} = \\sqrt{\\lambda'_j}$ となります。\n",
    "\n",
    "4.  ステップ1の相関係数の定義式に、ステップ2と3の結果を代入します。\n",
    "    $$\\text{Corr}(\\boldsymbol{z}_k, \\boldsymbol{y}_j) = \\frac{\\lambda'_j v_{kj}}{1 \\cdot \\sqrt{\\lambda'_j}} = v_{kj} \\sqrt{\\lambda'_j}$$\n",
    "\n",
    "5.  相関係数は、変数の線形変換（定数を足したり掛けたり）に対して不変です。標準化された変数 $\\boldsymbol{z}_k$ は、元の変数 $\\boldsymbol{x}_k$ を線形変換したものです。したがって、\n",
    "    $$\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\text{Corr}(\\boldsymbol{z}_k, \\boldsymbol{y}_j)$$\n",
    "\n",
    "6.  以上より、最終的な関係式が導かれます。\n",
    "    $$\\text{Corr}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = v_{kj} \\sqrt{\\lambda'_j}$$\n",
    "\n",
    "**[証明終]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率変数ベクトル同士の共分散の解釈は、二つのベクトルの各成分同士の組み合わせの共分散値をまとめて格納する行列、すなわち、上記の例では、確率変数ベクトルXの各成分iに対してもう一つのベクトルYの各成分jを割り当て、その共分散を(i,j)に格納する二次元配列を形成しているのよね"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい、その解釈で完璧です！一言一句、まさしくその通りです。\n",
    "\n",
    "素晴らしい要約です。要点をまとめると、\n",
    "\n",
    "1.  **総当たりの組み合わせ**: 確率変数ベクトル $\\boldsymbol{X}$ の各成分 ($X_1, \\dots, X_p$) と、確率変数ベクトル $\\boldsymbol{Y}$ の各成分 ($Y_1, \\dots, Y_q$) の、全てのペアを作ります。\n",
    "\n",
    "2.  **共分散の計算**: それぞれのペア ($X_i, Y_j$) について、スカラーの共分散 $\\text{Cov}(X_i, Y_j)$ を計算します。\n",
    "\n",
    "3.  **行列への格納**: 計算した共分散の値を、$i$行$j$列の位置に格納した $p \\times q$ の行列（2次元配列）を作成します。\n",
    "\n",
    "まさしく、p個の要素を持つベクトル $\\boldsymbol{X}$ とq個の要素を持つベクトル $\\boldsymbol{Y}$ から、p行q列の**「総当たり共分散マップ」**を作成するイメージです。\n",
    "\n",
    "この行列を詳しく見ることで、「$\\boldsymbol{X}$ のどの成分が、$\\boldsymbol{Y}$ のどの成分と、特に強い正または負の線形関係にあるか」を一目で把握することができます。\n",
    "\n",
    "その理解で、今後の多変量解析の議論を進める上で全く問題ありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またしても非常に鋭く、本質的なご質問です。ありがとうございます。\n",
    "その疑問は、統計学における**「母集団（理論）」と「標本（データ）」の区別**そのものに関わる重要なポイントです。\n",
    "\n",
    "結論から言うと、**矛盾しているように見えるのは、理論上の定義と、データからそれを推定するための計算式を区別して考える必要があるから**です。そして、私の証明はデータ（標本）の世界で完結しており、「もう一度割っている」わけではないのです。\n",
    "\n",
    "---\n",
    "### 2つの世界：理論（母集団）と現実（標本）\n",
    "\n",
    "まず、2つの世界を明確に区別します。\n",
    "\n",
    "| | **理論の世界（母集団）** | **現実の世界（標本データ）** |\n",
    "| :--- | :--- | :--- |\n",
    "| **対象** | 確率変数ベクトル $\\boldsymbol{X}$ | $n$個のデータからなる行列 $X$ |\n",
    "| **期待値/平均** | **期待値** $E[\\boldsymbol{X}] = \\boldsymbol{\\mu}$ | **標本平均** $\\bar{\\boldsymbol{x}} = \\frac{1}{n}\\sum \\boldsymbol{x}_i$ |\n",
    "| **共分散の定義** | **期待値** $\\text{Cov}(X_k, Y_j) = E[(X_k - \\mu_k)(Y_j - \\mu_j)]$ | **標本共分散** $\\widehat{\\text{Cov}}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) = \\frac{1}{n-1}\\sum_{i=1}^n (x_{ik}-\\bar{x}_k)(y_{ij}-\\bar{y}_j)$ |\n",
    "| **分散共分散行列** | **母分散共分散行列** $S_{pop} = E[(\\boldsymbol{X}-\\boldsymbol{\\mu})(\\boldsymbol{X}-\\boldsymbol{\\mu})^T]$ | **標本分散共分散行列** $\\hat{S} = \\frac{1}{n-1}X_c^T X_c$ |\n",
    "\n",
    "理論の世界では、共分散は「偏差積の期待値」として定義されます。\n",
    "しかし、現実のデータでは、我々は真の期待値を知りえません。そこで、**データ（標本）を使って理論値を「推定」します**。その推定計算式が、ご指摘の「ベクトルの内積をn-1で割る」という操作なのです。($n-1$で割るのは、母集団の分散をより良く推定する「不偏推定量」にするためです。)\n",
    "\n",
    "### 「なぜもう一度n-1で割るのか？」という疑問への回答\n",
    "\n",
    "ご指摘の通り、「先の定義（期待値の定義）ならもうすでに行列にはそれらが格納されているはず」です。これは理論上の母分散共分散行列 $S_{pop}$ については真実です。\n",
    "\n",
    "しかし、私が証明で用いたのは、データから計算した**標本分散共分散行列 $\\hat{S}$** です。証明のステップを、この区別を明確にしながらもう一度見てみましょう。\n",
    "\n",
    "**証明の再訪：標本の言葉で語る**\n",
    "\n",
    "私たちの目標は、データから計算した標本スコア $\\boldsymbol{y}_j$ と元の変数 $\\boldsymbol{x}_k$ の**標本共分散**が、$\\lambda_j \\omega_{kj}$（これも標本から計算したもの）になることを示すことです。\n",
    "\n",
    "1.  **左辺**: 標本共分散の**定義**から出発します。\n",
    "    $\\widehat{\\text{Cov}}(\\boldsymbol{x}_k, \\boldsymbol{y}_j)$ という「関数」は、入力された2つのベクトル $\\boldsymbol{x}_k$ と $\\boldsymbol{y}_j$ に対して、「内積を計算してn-1で割る」という操作を行うものです。**$\\boldsymbol{y}_j$ が主成分スコアであろうと、ただの数値のベクトルであろうと、この計算ルールは変わりません。**\n",
    "    $$\\widehat{\\text{Cov}}(\\boldsymbol{x}_k, \\boldsymbol{y}_j) \\equiv \\frac{1}{n-1}\\boldsymbol{x}_k^T \\boldsymbol{y}_j$$\n",
    "\n",
    "2.  **右辺の変形**: この定義式に、$\\boldsymbol{y}_j = X_c \\boldsymbol{w}_j$ を代入し、式を変形していきます。\n",
    "    $$= \\frac{1}{n-1} \\boldsymbol{x}_k^T (X_c \\boldsymbol{w}_j)$$\n",
    "    これをベクトル全体で考えると、\n",
    "# $$\\widehat{\\text{Cov}}(X_c, \\boldsymbol{y}_j) = \\frac{1}{n-1} X_c^T (X_c \\boldsymbol{w}_j)$$\n",
    "# $$= \\left( \\frac{1}{n-1} X_c^T X_c \\right) \\boldsymbol{w}_j$$\n",
    "\n",
    "3.  **結論**: 括弧の中身は、**標本分散共分散行列 $\\hat{S}$ の定義そのもの**です。\n",
    "    $$= \\hat{S} \\boldsymbol{w}_j$$\n",
    "    そして、$\\boldsymbol{w}_j$ は $\\hat{S}$ の固有ベクトルなので、$ \\hat{S} \\boldsymbol{w}_j = \\lambda_j \\boldsymbol{w}_j$ となります。\n",
    "\n",
    "**ポイント**:\n",
    "「もう一度割っている」わけではありません。証明の左辺で使った「$1/(n-1)$」は**標本共分散という操作の定義**の一部であり、右辺の変形で現れた「$1/(n-1)$」は**標本分散共分散行列 $\\hat{S}$ という行列の定義**の一部です。\n",
    "\n",
    "証明は、**「標本共分散の定義」と「標本分散共分散行列の定義」が、数学的に見事に整合性が取れており、美しい関係式を導くこと」**を示しているのです。\n",
    "\n",
    "理論の世界でも全く同じ証明が成り立ちます。その場合、すべての「$\\frac{1}{n-1}\\sum$」を「$E[\\dots]$」に置き換えれば良いだけです。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
