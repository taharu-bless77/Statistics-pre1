{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a79496e",
   "metadata": {},
   "source": [
    "「繰り返し期待値の法則（Law of Iterated Expectations）」**を核に据えて、超幾何分布の期待値の導出を再構築します。\n",
    "\n",
    "「くじ引きは公平だから」という一言で済ませるのではなく、**「過去のシナリオ（履歴）によって確率は変わるが、それらをすべて平均すると初期確率と同じになる」**というプロセスを数学的に記述します。\n",
    "\n",
    "---\n",
    "\n",
    "### 超幾何分布の期待値の導出（再定義）\n",
    "\n",
    "#### 1. 設定と定義\n",
    "* 全体：$N$ 個\n",
    "* 赤玉（アタリ）：$M$ 個\n",
    "* 抽出数：$K$ 回\n",
    "* 確率変数 $X$：抽出された赤玉の総数\n",
    "\n",
    "まず、全体の赤玉数 $X$ を、各試行ごとの「指示確率変数（Indicator Variable）」$X_i$ の和に分解します。\n",
    "\n",
    "$$X = X_1 + X_2 + \\cdots + X_K = \\sum_{i=1}^K X_i$$\n",
    "\n",
    "ここで、$X_i$ は $i$ 回目に赤玉が出れば $1$、出なければ $0$ をとる変数です。\n",
    "期待値の線形性より、\n",
    "$$E[X] = \\sum_{i=1}^K E[X_i]$$\n",
    "となります。したがって、**任意の $i$ 回目において $E[X_i] = \\frac{M}{N}$ となること**を、あなたの指摘した「シナリオ（履歴）の考慮」を用いて証明すれば完了です。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. $i$ 回目の期待値 $E[X_i]$ の証明（帰納的アプローチ）\n",
    "\n",
    "「$i-1$ 回目までの結果（シナリオ）」を条件として、確率がどう変動し、どう収束するかを確認します。\n",
    "\n",
    "**【ステップA： $i=1$ のとき】**\n",
    "誰も引いていないので、条件なしで単純に計算します。\n",
    "$$P(X_1=1) = \\frac{M}{N}$$\n",
    "よって、$E[X_1] = \\frac{M}{N}$ です。\n",
    "\n",
    "**【ステップB： $i$ 番目のとき】**\n",
    "ここがあなたの洞察の核心部分です。\n",
    "$i$ 番目に赤を引く確率は、**それまでの $i-1$ 回で「すでに赤が何個出たか」**（これを確率変数 $S_{i-1}$ とします）に依存します。\n",
    "\n",
    "$i$ 回目に赤が出る条件付き確率は、以下のように表せます。\n",
    "$$P(X_i=1 \\mid S_{i-1}) = \\frac{M - S_{i-1}}{N - (i-1)}$$\n",
    "* 分子：元々の赤 $M$ から、すでに出た赤 $S_{i-1}$ を引いたもの\n",
    "* 分母：元々の全体 $N$ から、すでに出た個数 $i-1$ を引いたもの\n",
    "\n",
    "**【ステップC： 全確率（期待値）の計算】**\n",
    "あなたの指摘通り、この「条件付き確率」の期待値をとります（繰り返し期待値の法則）。\n",
    "\n",
    "$$P(X_i=1) = E[\\, P(X_i=1 \\mid S_{i-1}) \\,]$$\n",
    "$$P(X_i=1) = E\\left[ \\frac{M - S_{i-1}}{N - (i-1)} \\right]$$\n",
    "\n",
    "期待値の線形性を使って、定数部分を外に出し、変数 $S_{i-1}$ に期待値記号をつけます。\n",
    "\n",
    "$$= \\frac{M - E[S_{i-1}]}{N - i + 1}$$\n",
    "\n",
    "ここで、**帰納法的な仮定**を使います。\n",
    "「もし、$i-1$ 回目までの抽出において、1回あたりの赤が出る期待値がすべて $M/N$ だったとしたら」、ここまでの合計 $S_{i-1}$ の期待値は次のようになります。\n",
    "$$E[S_{i-1}] = (i-1) \\times \\frac{M}{N}$$\n",
    "\n",
    "これを先ほどの式に代入します。\n",
    "\n",
    "$$P(X_i=1) = \\frac{M - (i-1)\\frac{M}{N}}{N - i + 1}$$\n",
    "\n",
    "分子を通分して整理します。\n",
    "$$\\text{分子} = \\frac{MN - M(i-1)}{N} = \\frac{M \\{N - (i-1)\\}}{N} = \\frac{M(N - i + 1)}{N}$$\n",
    "\n",
    "これを元の式に戻すと、分母の $(N-i+1)$ がきれいに約分されます。\n",
    "\n",
    "$$P(X_i=1) = \\frac{\\frac{M(N - i + 1)}{N}}{N - i + 1} = \\frac{M}{N}$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 結論： 全体の期待値 $E[X]$\n",
    "\n",
    "上記より、過去のどのようなシナリオ（$S_{i-1}$）を考慮しても、それを確率的に平均化すると、結局 $i$ 回目の期待値は常に一定になることが証明されました。\n",
    "\n",
    "$$E[X_i] = \\frac{M}{N} \\quad (\\text{すべての } i \\text{ について})$$\n",
    "\n",
    "したがって、全体の期待値はこれらを $K$ 回足し合わせたものになります。\n",
    "\n",
    "$$E[X] = \\sum_{i=1}^K E[X_i] = \\sum_{i=1}^K \\frac{M}{N} = K \\times \\frac{M}{N}$$\n",
    "\n",
    "期待値のときと同様に、あなたの**「シナリオ（条件付き確率）」**の視点を取り入れることで、分散の導出における**共分散（Covariance）**の項が非常にクリアに理解できます。\n",
    "\n",
    "分散の定義式からスタートして、順を追って導出します。\n",
    "\n",
    "---\n",
    "\n",
    "### 超幾何分布の分散の導出\n",
    "\n",
    "#### 1. 分散の基本式\n",
    "確率変数の和 $X = \\sum X_i$ の分散は、以下の式で表されます。\n",
    "$$V[X] = V\\left[ \\sum_{i=1}^K X_i \\right] = \\sum_{i=1}^K V[X_i] + \\sum_{i \\neq j} Cov(X_i, X_j)$$\n",
    "\n",
    "この式は2つのパートで構成されています。\n",
    "1.  **個別の分散の和**（対角成分）\n",
    "2.  **異なる試行間の共分散の和**（相互作用成分）\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. パート1：個別の分散 $V[X_i]$\n",
    "さきほど証明した通り、どの $i$ 回目であっても、過去のシナリオをすべて平均化した確率は $P(X_i=1) = \\frac{M}{N}$ でした。\n",
    "これを $p = \\frac{M}{N}$ と置きます。$X_i$ はベルヌーイ試行（0か1）なので、分散の公式より：\n",
    "\n",
    "$$V[X_i] = p(1-p) = \\frac{M}{N} \\left( 1 - \\frac{M}{N} \\right)$$\n",
    "\n",
    "これが $K$ 個あるので、合計は：\n",
    "$$\\sum_{i=1}^K V[X_i] = K \\cdot \\frac{M}{N} \\left( 1 - \\frac{M}{N} \\right)$$\n",
    "（これは二項分布の分散 $np(1-p)$ と同じ形です）\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. パート2：共分散 $Cov(X_i, X_j)$ 【重要】\n",
    "ここがあなたの「シナリオ視点」が活きる場所です。\n",
    "共分散の定義は以下の通りです。\n",
    "$$Cov(X_i, X_j) = E[X_i X_j] - E[X_i]E[X_j]$$\n",
    "\n",
    "このうち $E[X_i]E[X_j] = p^2$ は既知です。問題は $E[X_i X_j]$ です。\n",
    "\n",
    "**■ $E[X_i X_j]$ の計算**\n",
    "$X_i, X_j$ は $0$ か $1$ しかとらないため、$X_i X_j = 1$ となるのは「両方とも赤」のときだけです。つまり同時確率 $P(X_i=1 \\cap X_j=1)$ を求めればよいことになります。\n",
    "\n",
    "ここで、あなたの発想を使います。$i$ と $j$ の時間的順序に関わらず、**「片方（$i$）の結果をシナリオとして固定し、もう片方（$j$）の確率を考える」**ことができます。\n",
    "\n",
    "1.  まず、$X_i=1$ （赤）となる確率は？\n",
    "    $$P(X_i=1) = \\frac{M}{N}$$\n",
    "2.  **そのシナリオ（$X_i=1$）が起きたという条件下で**、$X_j$ も赤になる確率は？\n",
    "    赤玉が1つ減っているので、残りは $N-1$ 個中、赤 $M-1$ 個です。\n",
    "    $$P(X_j=1 \\mid X_i=1) = \\frac{M-1}{N-1}$$\n",
    "\n",
    "よって、同時確率（積率）は：\n",
    "$$E[X_i X_j] = \\frac{M}{N} \\times \\frac{M-1}{N-1}$$\n",
    "\n",
    "**■ 共分散の計算**\n",
    "これを共分散の式に代入します。\n",
    "$$Cov(X_i, X_j) = \\left( \\frac{M}{N} \\cdot \\frac{M-1}{N-1} \\right) - \\left( \\frac{M}{N} \\cdot \\frac{M}{N} \\right)$$\n",
    "\n",
    "共通因数 $\\frac{M}{N}$ でくくって通分します。\n",
    "$$= \\frac{M}{N} \\left( \\frac{M-1}{N-1} - \\frac{M}{N} \\right)$$\n",
    "$$= \\frac{M}{N} \\left( \\frac{N(M-1) - M(N-1)}{N(N-1)} \\right)$$\n",
    "$$= \\frac{M}{N} \\left( \\frac{NM - N - MN + M}{N(N-1)} \\right)$$\n",
    "$$= \\frac{M}{N} \\left( \\frac{-(N-M)}{N(N-1)} \\right) = -\\frac{M(N-M)}{N^2(N-1)}$$\n",
    "\n",
    "**【解釈】**\n",
    "共分散が「負」になりました。これは**「1つ赤が出ると（シナリオ）、残りの赤が減るため、別の場所で赤が出る確率は下がる」**という非復元抽出の性質を見事に表しています。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 全体をまとめる\n",
    "分散 $V[X]$ の式に戻ります。\n",
    "共分散の項は、$i \\neq j$ となる組み合わせの数だけあります。$K$ 個から異なる2つを選ぶ順列なので、$K(K-1)$ 個です。\n",
    "\n",
    "$$V[X] = \\underbrace{K \\cdot \\frac{M}{N} \\left( 1 - \\frac{M}{N} \\right)}_{\\text{各分散の和}} + \\underbrace{K(K-1) \\cdot \\left( -\\frac{M(N-M)}{N^2(N-1)} \\right)}_{\\text{共分散の和}}$$\n",
    "\n",
    "ここで、$1 - \\frac{M}{N} = \\frac{N-M}{N}$ と書き換えて、共通項 $K \\frac{M(N-M)}{N^2}$ でくくります。\n",
    "\n",
    "$$V[X] = K \\frac{M(N-M)}{N^2} \\left[ 1 - \\frac{K-1}{N-1} \\right]$$\n",
    "\n",
    "大括弧の中を通分します。\n",
    "$$1 - \\frac{K-1}{N-1} = \\frac{(N-1) - (K-1)}{N-1} = \\frac{N-K}{N-1}$$\n",
    "\n",
    "これらを合わせると、最終的な公式が得られます。\n",
    "\n",
    "$$V[X] = K \\cdot \\frac{M}{N} \\left( 1 - \\frac{M}{N} \\right) \\cdot \\frac{N-K}{N-1}$$\n",
    "\n",
    "### まとめ\n",
    "\n",
    "1.  **期待値**： $K \\frac{M}{N}$\n",
    "2.  **分散**： $K \\frac{M}{N} (1-\\frac{M}{N}) \\times \\frac{N-K}{N-1}$\n",
    "\n",
    "「シナリオ（条件）」の考え方は、**共分散の導出（$E[X_i X_j]$ の計算）において不可欠なステップ**でした。\n",
    "「赤が出た」というシナリオが発生することで、次の確率が下がる（共分散が負になる）。この負の共分散が累積することで、全体の分散は二項分布（復元抽出）のときよりも小さくなります（修正係数 $\\frac{N-K}{N-1}$ が1より小さくなるため）。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
